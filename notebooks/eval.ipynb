{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/asaf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15bedeb70>"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union, List\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import tokenize\n",
    "from nltk.translate import bleu_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import textwrap\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    TFGPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from data.dataset import ComplexUtteranceCodeDataset\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "WORK_AREA = '..'\n",
    "os.chdir(WORK_AREA)\n",
    "\n",
    "paths = ['./src/', './src/api/v6', './notebooks/src']\n",
    "for path in paths:\n",
    "    path = os.path.normcase(path)\n",
    "    if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ModelFlavour(Enum):\n",
    "    Text2Code = \"text2code\"\n",
    "    Text2Rep = \"text2rep\"\n",
    "    Rep2Code = \"rep2code\"\n",
    "    Rep2Rep = \"rep2rep\"\n",
    "    TextRep2Rep = \"textrep2rep\"\n",
    "    TextRep2Code = \"textrep2code\"\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    T5Base = \"t5-base\"\n",
    "    CodeT5Small = \"codet5-small\"\n",
    "    CodeT5Base = \"codet5-base\"\n",
    "    CodeT5P220m = \"codet5p-220m\"\n",
    "    GPT2Small = \"gpt2-small\"\n",
    "    GPT2Medium = \"gpt2-medium\"\n",
    "\n",
    "\n",
    "model_flavour_params = {\n",
    "    ModelFlavour.Text2Code: dict(\n",
    "        slug = \"text2code\",\n",
    "        input_prefix = \"text to code: \",\n",
    "        input_label = \"text\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "    ModelFlavour.Text2Rep: dict(\n",
    "        slug = \"text2rep\",\n",
    "        input_prefix = \"text to rep: \",\n",
    "        input_label = \"text\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.Rep2Code: dict(\n",
    "        slug = \"rep2code\",\n",
    "        input_prefix = \"rep to code: \",\n",
    "        input_label = \"lang_rep\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "    ModelFlavour.Rep2Rep: dict(\n",
    "        slug = \"rep2rep\",\n",
    "        input_prefix = \"rep to rep: \",\n",
    "        input_label = \"lang_rep\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.TextRep2Rep: dict(\n",
    "        slug = \"text_rep2rep\",\n",
    "        input_prefix = \"text and rep to rep: \",\n",
    "        input_label = \"text_lang_rep\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.TextRep2Code: dict(\n",
    "        slug = \"textrep2code\",\n",
    "        input_prefix = \"text and rep to code: \",\n",
    "        input_label = \"text_lang_rep\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "pretrained_model_names_mapping = {\n",
    "    Model.T5Base: \"t5-base\",\n",
    "    Model.CodeT5Small: \"Salesforce/codet5-small\",\n",
    "    Model.CodeT5Base: \"Salesforce/codet5-base\",\n",
    "    Model.CodeT5P220m: \"Salesforce/codet5p-220m\",\n",
    "    Model.GPT2Small: \"gpt2\",\n",
    "    Model.GPT2Medium: \"gpt2-medium\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ModelFlavour(Enum):\n",
    "    Text2Code = \"text2code\"\n",
    "    Text2Rep = \"text2rep\"\n",
    "    Rep2Code = \"rep2code\"\n",
    "    Rep2Rep = \"rep2rep\"\n",
    "    TextRep2Rep = \"textrep2rep\"\n",
    "    TextRep2Code = \"textrep2code\"\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    T5Base = \"t5-base\"\n",
    "    CodeT5Small = \"codet5-small\"\n",
    "    CodeT5Base = \"codet5-base\"\n",
    "    CodeT5P220m = \"codet5p-220m\"\n",
    "    GPT2Small = \"gpt2-small\"\n",
    "    GPT2Medium = \"gpt2-medium\"\n",
    "\n",
    "\n",
    "model_flavour_params = {\n",
    "    ModelFlavour.Text2Code: dict(\n",
    "        slug = \"text2code\",\n",
    "        input_prefix = \"text to code: \",\n",
    "        input_label = \"text\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "    ModelFlavour.Text2Rep: dict(\n",
    "        slug = \"text2rep\",\n",
    "        input_prefix = \"text to rep: \",\n",
    "        input_label = \"text\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.Rep2Code: dict(\n",
    "        slug = \"rep2code\",\n",
    "        input_prefix = \"rep to code: \",\n",
    "        input_label = \"lang_rep\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "    ModelFlavour.Rep2Rep: dict(\n",
    "        slug = \"rep2rep\",\n",
    "        input_prefix = \"rep to rep: \",\n",
    "        input_label = \"lang_rep\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.TextRep2Rep: dict(\n",
    "        slug = \"text_rep2rep\",\n",
    "        input_prefix = \"text and rep to rep: \",\n",
    "        input_label = \"text_lang_rep\",\n",
    "        target_label = \"code_rep\",\n",
    "    ),\n",
    "    ModelFlavour.TextRep2Code: dict(\n",
    "        slug = \"textrep2code\",\n",
    "        input_prefix = \"text and rep to code: \",\n",
    "        input_label = \"text_lang_rep\",\n",
    "        target_label = \"code\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "pretrained_model_names_mapping = {\n",
    "    Model.T5Base: \"t5-base\",\n",
    "    Model.CodeT5Small: \"Salesforce/codet5-small\",\n",
    "    Model.CodeT5Base: \"Salesforce/codet5-base\",\n",
    "    Model.CodeT5P220m: \"Salesforce/codet5p-220m\",\n",
    "    Model.GPT2Small: \"gpt2\",\n",
    "    Model.GPT2Medium: \"gpt2-medium\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_tokenizer(model_flavour: ModelFlavour, pretrained_model_name_or_path: str):\n",
    "  if model_flavour in [Model.T5Base]:\n",
    "    return T5Tokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "  elif model_flavour in [Model.CodeT5Small, Model.CodeT5Base, Model.CodeT5P220m]:\n",
    "    return RobertaTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "  elif model_flavour in [Model.GPT2Small]:\n",
    "    return OpenAIGPTTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "  elif model_flavour in [Model.GPT2Medium]:\n",
    "    return GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "  else:\n",
    "    raise ValueError(f\"No such model flavour {model_flavour}\")\n",
    "\n",
    "\n",
    "def load_model(model_flavour: ModelFlavour, pretrained_model_name_or_path: str):\n",
    "  if model_flavour in [Model.T5Base, Model.CodeT5Small, Model.CodeT5Base, Model.CodeT5P220m]:\n",
    "    return T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path)\n",
    "  elif model_flavour in [Model.GPT2Small, Model.GPT2Medium]:\n",
    "    return TFGPT2Model.from_pretrained(pretrained_model_name_or_path)\n",
    "  else:\n",
    "    raise ValueError(f\"No such model flavour {model_flavour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, TypeVar, Generic\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import glob\n",
    "from representations.tree.tree import Tree\n",
    "from representations.builders.ast.tearers.tearer_factory import TearerFactory\n",
    "import tokenize\n",
    "from nltk.translate import bleu_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def parse_code_rep_to_code(code_rep: str, verbose: str = \"Fatal\") -> str:\n",
    "    try:\n",
    "        tree = Tree.unparse(code_rep)\n",
    "        tearer = TearerFactory().get_tearer(tree.root_node)\n",
    "        asdl = tearer.tear(tree.root_node)\n",
    "        code = ast.unparse(asdl)\n",
    "    except Exception as e:\n",
    "        if verbose == \"Error\":\n",
    "            print(f\"[Error] failed to prase code rep to code:\\n\", e)\n",
    "        code = \"\"\n",
    "    finally:\n",
    "        return code\n",
    "\n",
    "\n",
    "def build_test_code(\n",
    "    code: str,\n",
    "    imports: str,\n",
    "    test: str,\n",
    "    code_embed_str: str = \"# end code block to test\",\n",
    "    fail_on_error: bool = False,\n",
    "    verbose: str = \"Fatal\",\n",
    "):\n",
    "    try:\n",
    "        code_insert_idx = test.find(code_embed_str)\n",
    "        program_code = imports\n",
    "        program_code += \"\\n\"\n",
    "        program_code += test[:code_insert_idx]\n",
    "        program_code += code\n",
    "        program_code += \"\\n\"\n",
    "        program_code += test[code_insert_idx:]\n",
    "    except Exception as e:\n",
    "        if verbose == \"Error\":\n",
    "            print(\"[ERROR] Failed to unparse code rep to code\\n\", e)\n",
    "        if fail_on_error:\n",
    "            raise e\n",
    "        program_code = \"\"\n",
    "    finally:\n",
    "        return program_code\n",
    "\n",
    "\n",
    "def tokenize_source(code):\n",
    "    file_path = \"/tmp/example.py\"\n",
    "\n",
    "    with open(file_path, \"w\") as text_file:\n",
    "        text_file.write(code)\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        tokens_gen = tokenize.tokenize(f.readline)\n",
    "\n",
    "        tokens = [token.string for token in tokens_gen]\n",
    "\n",
    "    os.remove(file_path)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def eval_code(code: str):\n",
    "    test_results = {}\n",
    "    try:\n",
    "        context = {}\n",
    "        exec(code, context)\n",
    "        test_results = context.get(\"test_results\", {})\n",
    "        test_results[\"execution_success\"] = test_results.get(\"execution_success\", 0) + 1\n",
    "    except AssertionError as e:\n",
    "        test_results[\"assertion_failure\"] = test_results.get(\"assertion_failure\", 0) + 1\n",
    "    except Exception as e:\n",
    "        test_results[\"execution_failure\"] = test_results.get(\"execution_failure\", 0) + 1\n",
    "\n",
    "    code_failure = test_results.get(\"code_failure\", 0)\n",
    "    assertion_failure = test_results.get(\"assertion_failure\", 0)\n",
    "    execution_failure = test_results.get(\"execution_failure\", 0)\n",
    "    execution_success = test_results.get(\"execution_success\", 0)\n",
    "    correct = test_results.get(\"correct\", 0)\n",
    "    incorrect = test_results.get(\"incorrect\", 0)\n",
    "    total = (correct + incorrect) or math.inf\n",
    "    accuracy = (1 - code_failure) * (correct / total)\n",
    "\n",
    "    results = dict(\n",
    "        code_failure=code_failure,\n",
    "        execution_success=execution_success,\n",
    "        execution_failure=execution_failure,\n",
    "        assertion_failure=assertion_failure,\n",
    "        correct=correct,\n",
    "        incorrect=incorrect,\n",
    "        accuracy=accuracy,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_predictions(\n",
    "    model, tokenizer, dataloader, gold_column, id_labels, max_length, k\n",
    "):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    ks = []\n",
    "    ids = {}\n",
    "    for id_label in id_labels:\n",
    "        ids[id_label] = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        outs = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=k>1,\n",
    "            num_return_sequences=k\n",
    "        )\n",
    "\n",
    "        output = [tokenizer.decode(out, skip_special_tokens=True) for out in outs]\n",
    "        target = [t.strip() for t in list(np.repeat(batch[gold_column], k))]\n",
    "\n",
    "        outputs.extend(output)\n",
    "        targets.extend(target)\n",
    "        ks.extend(list(np.arange(k)) * (batch[\"input_ids\"].shape[0]))\n",
    "        for id_label in id_labels:\n",
    "            ids[id_label].extend(list(np.repeat(batch[id_label], k)))\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        **{\n",
    "            \"output\": outputs,\n",
    "            \"target\": targets,\n",
    "            \"k\": ks,\n",
    "        },\n",
    "        **ids\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def eval_model(data: pd.DataFrame):\n",
    "    results = dict(\n",
    "        exact=metrics.accuracy_score(data[\"target\"], data[\"output\"]),\n",
    "        bleu=None,\n",
    "        humaneval=eval_model_humaneval(data[\"target\"], data[\"output\"]),\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def humaneval_accuracy_score(\n",
    "    data: pd.DataFrame,\n",
    "    code_column_name: str = \"pred_code\",\n",
    "    score_id_labels1: Union[str, List[str]] = [\"sample_id\", \"k\"],\n",
    "    score_id_labels2: Union[str, List[str]] = \"sample_id\",\n",
    "    score_column_name: str = \"accuracy\",\n",
    "):\n",
    "    test_codes = data.apply(\n",
    "        lambda x: build_test_code(\n",
    "            code=x[code_column_name], imports=x[\"imports\"], test=x[\"test\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    test_results = test_codes.apply(lambda test_code: eval_code(test_code))\n",
    "    test_results_df = pd.DataFrame.from_records(\n",
    "        test_results.values, index=test_results.index\n",
    "    )\n",
    "    test_scores = (\n",
    "        test_results_df.reset_index(drop=False)\n",
    "        .groupby(score_id_labels1)[score_column_name]\n",
    "        .mean()\n",
    "    )\n",
    "    score = (\n",
    "        test_scores.reset_index(drop=False)\n",
    "        .groupby(score_id_labels2)[score_column_name]\n",
    "        .max()\n",
    "        .mean()\n",
    "    )\n",
    "    return dict(score=score, results=test_results_df)\n",
    "\n",
    "\n",
    "def bleu_accuracy_score(\n",
    "    data: pd.DataFrame,\n",
    "    generated_column=\"output\",\n",
    "    gold_column=\"code\",\n",
    "    score_id_labels1: Union[str, List[str]] = [\"sample_id\", \"k\"],\n",
    "    score_id_labels2: Union[str, List[str]] = \"sample_id\",\n",
    "    score_column_name: str = \"bleu_score\",\n",
    "):\n",
    "    eval_results = data.apply(\n",
    "        lambda x: eval_bleu(x[gold_column], x[generated_column]), axis=1\n",
    "    )\n",
    "    eval_results_df = eval_results.to_frame(\"bleu_score\")\n",
    "    test_scores = (\n",
    "        eval_results_df.reset_index(drop=False)\n",
    "        .groupby(score_id_labels1)[score_column_name]\n",
    "        .mean()\n",
    "    )\n",
    "    score = (\n",
    "        test_scores.reset_index(drop=False)\n",
    "        .groupby(score_id_labels2)[score_column_name]\n",
    "        .max()\n",
    "        .mean()\n",
    "    )\n",
    "    return dict(score=score, results=eval_results_df)\n",
    "\n",
    "\n",
    "def model_eval(\n",
    "    results_df=None,\n",
    "    results_file_path=None,\n",
    "    output_column=\"output\",\n",
    "    gold_column=\"code\",\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=True,\n",
    "):\n",
    "    results_df = (\n",
    "        pd.read_csv(results_file_path) if results_file_path else results_df.copy()\n",
    "    )\n",
    "    results_df[\"sample_id\"] = results_df[\"sample_id\"].astype(int)\n",
    "    results_df.set_index([\"sample_id\", \"sample_minor_id\", \"k\"], inplace=True)\n",
    "    results_df.sort_index(inplace=True)\n",
    "\n",
    "    code_column = \"generated_code\"\n",
    "    if parse_to_code:\n",
    "        results_df[code_column] = results_df[output_column].apply(\n",
    "            lambda x: parse_code_rep_to_code(x)\n",
    "        )\n",
    "    else:\n",
    "        results_df[code_column] = results_df[output_column]\n",
    "\n",
    "    results_df[\"test\"] = results_df[\"test\"].str.replace(\n",
    "        \"= next(iterator)\", \"= next(iterator, None)\"\n",
    "    )\n",
    "    results_df[code_column] = results_df[code_column].str.replace(\n",
    "        \" = ContentType.\", \" = MessageContentType.\"\n",
    "    )\n",
    "    results_df[code_column] = results_df[code_column].str.replace(\n",
    "        \"Message.\", \"Messages.\"\n",
    "    )\n",
    "\n",
    "    humaneval_results = (\n",
    "        humaneval_accuracy_score(data=results_df, code_column_name=code_column)\n",
    "        if compute_humanval\n",
    "        else {}\n",
    "    )\n",
    "\n",
    "    bleu_results = (\n",
    "        bleu_accuracy_score(\n",
    "            data=results_df, generated_column=code_column, gold_column=gold_column\n",
    "        )\n",
    "        if compute_bleu\n",
    "        else {}\n",
    "    )\n",
    "\n",
    "    results = dict(humaneval=humaneval_results, bleu=bleu_results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_model_humaneval(\n",
    "    data: pd.DataFrame,\n",
    "    code_column_name: str = \"pred_code\",\n",
    "    score_id_labels: Union[str, List[str]] = \"sample_id\",\n",
    "    score_column_name: str = \"accuracy\",\n",
    "):\n",
    "    test_codes = data.apply(\n",
    "        lambda x: build_test_code(\n",
    "            code=x[code_column_name], imports=x[\"imports\"], test=x[\"test\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    test_results = test_codes.apply(lambda test_code: eval_code(test_code))\n",
    "\n",
    "    test_results_df = pd.DataFrame.from_records(\n",
    "        test_results.values, index=test_results.index\n",
    "    )\n",
    "    score = (\n",
    "        test_results_df.reset_index(drop=False)\n",
    "        .groupby(score_id_labels)[score_column_name]\n",
    "        .mean()\n",
    "        .mean()\n",
    "    )\n",
    "    return score, test_results_df\n",
    "\n",
    "\n",
    "def eval_bleu(code, generated_code):\n",
    "    if not code or not generated_code:\n",
    "        return 0\n",
    "\n",
    "    hypothesis = tokenize_source(code)\n",
    "\n",
    "    try:\n",
    "        reference = tokenize_source(generated_code)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    n = max(min(len(hypothesis), 4), 1)\n",
    "    weight = 1 / n\n",
    "    weights = (weight,) * n\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "    score = bleu_score.sentence_bleu(\n",
    "        [reference], hypothesis, weights=weights, smoothing_function=smoothing_function\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "def eval_generated_code(\n",
    "    df,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    k,\n",
    "    dataloader,\n",
    "    target_label,\n",
    "    id_labels,\n",
    "    max_length,\n",
    "    output_column=\"output\",\n",
    "    gold_column=\"code\",\n",
    "    parse_code=False,\n",
    "    file_path=None,\n",
    "    should_generate_predictions=True,\n",
    "    should_model_eval=True,\n",
    "):\n",
    "    file_exists = file_path and os.path.exists(file_path)\n",
    "    if should_generate_predictions and not file_exists:\n",
    "        preds_df = generate_predictions(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            k=k,\n",
    "            dataloader=dataloader,\n",
    "            gold_column=target_label,\n",
    "            id_labels=id_labels,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        if file_path:\n",
    "            results_df = df.join(preds_df.set_index(df.index.names))\n",
    "            results_df.to_csv(file_path)\n",
    "            print(f\"Results were saved to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Loading results from {file_path}\")\n",
    "        results_df = pd.read_csv(file_path)\n",
    "\n",
    "    results = None\n",
    "    if should_model_eval:\n",
    "        results = model_eval(\n",
    "            results_df=results_df,\n",
    "            parse_to_code=parse_code,\n",
    "            compute_humanval=True,\n",
    "            compute_bleu=True,\n",
    "            output_column=output_column,\n",
    "            gold_column=gold_column,\n",
    "        )\n",
    "        print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "        print(f\"bleu = {results['bleu']['score']}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape =  (152, 12)\n",
      "test_df (152, 12)\n",
      "test_df (152, 12)\n"
     ]
    }
   ],
   "source": [
    "from data.utils import (\n",
    "    get_dataset_args,\n",
    "    load_test_data,\n",
    ")\n",
    "\n",
    "test_file_path = 'build/eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz'\n",
    "test_df = load_test_data(test_file_path=test_file_path, id_labels=None)\n",
    "print(\"test_df\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asaf/Downloads/codet5-base-rep2rep-2023-05-24_122620\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "# args = dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='./experiments/codet5-base-text2code-2023-05-25_125337')\n",
    "args = dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='/Users/asaf/Downloads/codet5-base-rep2rep-2023-05-24_122620')\n",
    "# args = dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2Rep, pretrained_model_path='/Users/asaf/Downloads/codet5-base-text2rep-2023-05-24_143609')\n",
    "pretrained_model_path = args.get('pretrained_model_path')\n",
    "selected_model_type = args.get('selected_model_type')\n",
    "model_name = args.get('model_name')\n",
    "print(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id = codet5-base\n",
      "slug = rep2rep\n",
      "k = 10\n"
     ]
    }
   ],
   "source": [
    "# create a tokenizer and load the model\n",
    "tokenizer = load_tokenizer(\n",
    "  model_flavour=model_name,\n",
    "  pretrained_model_name_or_path=pretrained_model_names_mapping[model_name]\n",
    ")\n",
    "model = load_model(\n",
    "  model_flavour=model_name,\n",
    "  pretrained_model_name_or_path=pretrained_model_path\n",
    ")\n",
    "\n",
    "# selected model params\n",
    "selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
    "target_label = selected_model_flavour_params.get('target_label')\n",
    "parse_code = (target_label == 'code_rep')\n",
    "slug = selected_model_flavour_params.get('slug')\n",
    "\n",
    "# load the dataset\n",
    "dataset_args = get_dataset_args(tokenizer, selected_model_flavour_params)\n",
    "max_length = dataset_args['max_target_length']\n",
    "\n",
    "test_dataset = ComplexUtteranceCodeDataset(data=test_df, **dataset_args)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=2)\n",
    "\n",
    "model_id = model_name.value\n",
    "pretrained_model_file = [x for x in pretrained_model_path.split('/') if x][-1]\n",
    "test_results_file_path = f\"/Users/asaf/Downloads/results/test-{str(test_df.shape[0])}-{pretrained_model_file}-k{k}.csv.gz\"\n",
    "id_labels = ['test_id', 'sample_id', 'sample_minor_id']\n",
    "\n",
    "print(f\"model_id = {model_id}\")\n",
    "print(f\"slug = {slug}\")\n",
    "print(f\"k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b704fa6c4444ad856bcfcc65c3c9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = test_dataloader\n",
    "gold_column = target_label\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "ks = []\n",
    "ids = {}\n",
    "for id_label in id_labels:\n",
    "    ids[id_label] = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    outs = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        do_sample=k>1,\n",
    "        num_return_sequences=k\n",
    "    )\n",
    "\n",
    "    output = [tokenizer.decode(out, skip_special_tokens=True) for out in outs]\n",
    "    target = [t.strip() for t in list(np.repeat(batch[gold_column], k))]\n",
    "\n",
    "    outputs.extend(output)\n",
    "    targets.extend(target)\n",
    "    ks.extend(list(np.arange(k)) * (batch[\"input_ids\"].shape[0]))\n",
    "    for id_label in id_labels:\n",
    "        ids[id_label].extend(list(np.repeat(batch[id_label], k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({\n",
    "    **{\n",
    "        \"output\": outputs,\n",
    "        \"target\": targets,\n",
    "        \"k\": ks,\n",
    "    },\n",
    "    **ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/asaf/Downloads/results/test-152-codet5-base-rep2rep-2023-05-24_122620-k10.csv.gz'"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results were saved to /Users/asaf/Downloads/results/test-152-codet5-base-rep2rep-2023-05-24_122620-k10.csv.gz\n"
     ]
    }
   ],
   "source": [
    "df = test_df.set_index(id_labels)\n",
    "file_path=test_results_file_path\n",
    "\n",
    "results_df = df.join(preds_df.set_index(df.index.names))\n",
    "results_df.to_csv(file_path)\n",
    "print(f\"Results were saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.08196721311475409\n",
      "bleu = 0.34636317125504384\n"
     ]
    }
   ],
   "source": [
    "output_column = 'output'\n",
    "results = model_eval(\n",
    "    results_df=results_df2,\n",
    "    parse_to_code=parse_code,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=True,\n",
    "    output_column=output_column,\n",
    "    gold_column=gold_column,\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biu] *",
   "language": "python",
   "name": "conda-env-biu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
