{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmtLl76F8A8B"
      },
      "source": [
        "# Eval Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lteNoO4kIahi"
      },
      "source": [
        "Based on https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb#scrollTo=wvRHDkCIS91f and https://colab.research.google.com/drive/1d4xNsZbDSZ5ZqXgZjy7HyTVRLBJBVsh6#scrollTo=SDVQ04fGRb1v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-7bq0hWR3ny"
      },
      "source": [
        "## Set-up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRTrTycCMiDk"
      },
      "source": [
        "Let's first install the required libraries:\n",
        "* HuggingFace Transformers (for the CodeT5 model)\n",
        "* HuggingFace Datasets (for loading the dataset + preprocessing it)\n",
        "* PyTorch Lightning (for training)\n",
        "* Weights and Biases (for logging training metrics).\n",
        "* Project code from a GitHub repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i8n4pEe2RXFB"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers sentencepiece pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8IoXMEQx4gF",
        "outputId": "ac5b5598-c293-4096-a84a-f0d338ee5acf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into '/Users/asaf/tmp/complex-utterance-to-code'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[36mcomplex-utterance-to-code\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "rm -r ~/tmp/complex-utterance-to-code\n",
        "git clone https://github.com/asafam/novicode.git ~/tmp/complex-utterance-to-code\n",
        "ls ~/tmp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qtrgijePOeB6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "paths = [\n",
        "  '~/tmp/complex-utterance-to-code',\n",
        "  '~/tmp/complex-utterance-to-code/notebooks/src',\n",
        "  '~/tmp/complex-utterance-to-code/src',\n",
        "  '~/tmp/complex-utterance-to-code/src/api/v6',\n",
        "]\n",
        "for path in paths:\n",
        "  path = os.path.normcase(path)\n",
        "  if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
        "      sys.path.append(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItWCqHFnMV28",
        "outputId": "9763c59b-2083-466f-9b68-47aae3f8c23b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "WORK_DRIVE = '/Users/asaf/tmp'\n",
        "WORK_AREA = WORK_DRIVE + '/complex-utterance-to-code'\n",
        "\n",
        "# drive.mount(WORK_DRIVE)\n",
        "os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/Users/asaf/tmp/complex-utterance-to-code/src')\n",
        "sys.path.append('/Users/asaf/tmp/complex-utterance-to-code/notebooks/src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdaMbQFUMB-9",
        "outputId": "6c794373-0004-4c61-a79f-a7dd1cd01d1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/asaf/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x29132be30>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import Union, List\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "import tokenize\n",
        "from nltk.translate import bleu_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "import textwrap\n",
        "from sklearn import metrics\n",
        "import statistics\n",
        "\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    TFGPT2Model,\n",
        "    GPT2Tokenizer,\n",
        "    OpenAIGPTTokenizer,\n",
        "    RobertaTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from data.dataset import ComplexUtteranceCodeDataset\n",
        "from data.utils import (\n",
        "    get_dataset_args,\n",
        "    load_test_data,\n",
        ")\n",
        "from eval.utils import (\n",
        "    eval_generated_code,\n",
        "    model_eval\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu50z1gUIahm"
      },
      "source": [
        "### Model configuration code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HioPvr2JIahm"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class ModelFlavour(Enum):\n",
        "    Text2Code = \"text2code\"\n",
        "    Text2Rep = \"text2rep\"\n",
        "    Rep2Code = \"rep2code\"\n",
        "    Rep2Rep = \"rep2rep\"\n",
        "    RepRaw2RepRaw = \"repraw2repraw\"\n",
        "    Text2RepRaw = \"text2repraw\"\n",
        "    RepRaw2Code = \"repraw2code\"\n",
        "    TextRep2Rep = \"textrep2rep\"\n",
        "    TextRep2Code = \"textrep2code\"\n",
        "\n",
        "\n",
        "class Model(Enum):\n",
        "    T5Base = \"t5-base\"\n",
        "    CodeT5Small = \"codet5-small\"\n",
        "    CodeT5Base = \"codet5-base\"\n",
        "    CodeT5P220m = \"codet5p-220m\"\n",
        "    GPT2Small = \"gpt2-small\"\n",
        "    GPT2Medium = \"gpt2-medium\"\n",
        "\n",
        "\n",
        "model_flavour_params = {\n",
        "    ModelFlavour.Text2Code: dict(\n",
        "        slug = \"text2code\",\n",
        "        input_prefix = \"text to code: \",\n",
        "        input_label = \"text\",\n",
        "        target_label = \"code\",\n",
        "    ),\n",
        "    ModelFlavour.Text2Rep: dict(\n",
        "        slug = \"text2rep\",\n",
        "        input_prefix = \"text to rep: \",\n",
        "        input_label = \"text\",\n",
        "        target_label = \"code_rep\",\n",
        "    ),\n",
        "    ModelFlavour.Rep2Code: dict(\n",
        "        slug = \"rep2code\",\n",
        "        input_prefix = \"rep to code: \",\n",
        "        input_label = \"lang_rep\",\n",
        "        target_label = \"code\",\n",
        "    ),\n",
        "    ModelFlavour.Rep2Rep: dict(\n",
        "        slug = \"rep2rep\",\n",
        "        input_prefix = \"rep to rep: \",\n",
        "        input_label = \"lang_rep\",\n",
        "        target_label = \"code_rep\",\n",
        "    ),\n",
        "    ModelFlavour.TextRep2Rep: dict(\n",
        "        slug = \"text_rep2rep\",\n",
        "        input_prefix = \"text and rep to rep: \",\n",
        "        input_label = \"text_lang_rep\",\n",
        "        target_label = \"code_rep\",\n",
        "    ),\n",
        "    ModelFlavour.TextRep2Code: dict(\n",
        "        slug = \"textrep2code\",\n",
        "        input_prefix = \"text and rep to code: \",\n",
        "        input_label = \"text_lang_rep\",\n",
        "        target_label = \"code\",\n",
        "    ),\n",
        "    ModelFlavour.RepRaw2RepRaw: dict(\n",
        "        slug = \"repraw2repraw\",\n",
        "        input_prefix = \"raw rep to raw rep: \",\n",
        "        input_label = \"lang_rep_raw\",\n",
        "        target_label = \"code_rep_raw\",\n",
        "    ),\n",
        "    ModelFlavour.Text2RepRaw: dict(\n",
        "        slug = \"text2repraw\",\n",
        "        input_prefix = \"text to raw rep: \",\n",
        "        input_label = \"text\",\n",
        "        target_label = \"code_rep_raw\",\n",
        "    ),\n",
        "    ModelFlavour.RepRaw2Code: dict(\n",
        "        slug = \"repraw2code\",\n",
        "        input_prefix = \"raw rep to code: \",\n",
        "        input_label = \"lang_rep_raw\",\n",
        "        target_label = \"code\",\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "pretrained_model_names_mapping = {\n",
        "    Model.T5Base: \"t5-base\",\n",
        "    Model.CodeT5Small: \"Salesforce/codet5-small\",\n",
        "    Model.CodeT5Base: \"Salesforce/codet5-base\",\n",
        "    Model.CodeT5P220m: \"Salesforce/codet5p-220m\",\n",
        "    Model.GPT2Small: \"gpt2\",\n",
        "    Model.GPT2Medium: \"gpt2-medium\",\n",
        "}\n",
        "\n",
        "\n",
        "def load_tokenizer(model_flavour: ModelFlavour, pretrained_model_name_or_path: str):\n",
        "  if model_flavour in [Model.T5Base]:\n",
        "    return T5Tokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "  elif model_flavour in [Model.CodeT5Small, Model.CodeT5Base, Model.CodeT5P220m]:\n",
        "    return RobertaTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "  elif model_flavour in [Model.GPT2Small]:\n",
        "    return OpenAIGPTTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "  elif model_flavour in [Model.GPT2Medium]:\n",
        "    return GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "  else:\n",
        "    raise ValueError(f\"No such model flavour {model_flavour}\")\n",
        "\n",
        "\n",
        "def load_model(model_flavour: ModelFlavour, pretrained_model_name_or_path: str):\n",
        "  if model_flavour in [Model.T5Base, Model.CodeT5Small, Model.CodeT5Base, Model.CodeT5P220m]:\n",
        "    return T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path)\n",
        "  elif model_flavour in [Model.GPT2Small, Model.GPT2Medium]:\n",
        "    return TFGPT2Model.from_pretrained(pretrained_model_name_or_path)\n",
        "  else:\n",
        "    raise ValueError(f\"No such model flavour {model_flavour}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUtACTQUw1sK"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CHZwM0rNOhdD"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Optional, TypeVar, Generic\n",
        "import os\n",
        "import pandas as pd\n",
        "import ast\n",
        "import math\n",
        "import glob\n",
        "from representations.tree.tree import Tree\n",
        "from representations.builders.ast.tearers.tearer_factory import TearerFactory\n",
        "import tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from sklearn import metrics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def parse_code_rep_to_code(code_rep: str, rules_enabled: bool = False, verbose: str = \"Fatal\") -> str:\n",
        "    try:\n",
        "        tree = Tree.unparse(code_rep)\n",
        "        tearer = TearerFactory().get_tearer(tree.root_node, rules_enabled=rules_enabled)\n",
        "        asdl = tearer.tear(tree.root_node)\n",
        "        code = ast.unparse(asdl)\n",
        "    except Exception as e:\n",
        "        if verbose == \"Error\":\n",
        "            print(f\"[Error] failed to prase code rep to code:\\n\", e)\n",
        "        code = \"\"\n",
        "    finally:\n",
        "        return code\n",
        "\n",
        "\n",
        "def build_test_code(\n",
        "    code: str,\n",
        "    imports: str,\n",
        "    test: str,\n",
        "    code_embed_str: str = \"# end code block to test\",\n",
        "    fail_on_error: bool = False,\n",
        "    verbose: str = \"Fatal\",\n",
        "):\n",
        "    try:\n",
        "        code_insert_idx = test.find(code_embed_str)\n",
        "        program_code = imports\n",
        "        program_code += \"\\n\"\n",
        "        program_code += test[:code_insert_idx]\n",
        "        program_code += code\n",
        "        program_code += \"\\n\"\n",
        "        program_code += test[code_insert_idx:]\n",
        "    except Exception as e:\n",
        "        if verbose == \"Error\":\n",
        "            print(\"[ERROR] Failed to unparse code rep to code\\n\", e)\n",
        "        if fail_on_error:\n",
        "            raise e\n",
        "        program_code = \"\"\n",
        "    finally:\n",
        "        return program_code\n",
        "\n",
        "\n",
        "def tokenize_source(code):\n",
        "    file_path = \"/tmp/example.py\"\n",
        "\n",
        "    with open(file_path, \"w\") as text_file:\n",
        "        text_file.write(code)\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        tokens_gen = tokenize.tokenize(f.readline)\n",
        "\n",
        "        tokens = [token.string for token in tokens_gen]\n",
        "\n",
        "    os.remove(file_path)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def eval_code(code: str):\n",
        "    test_results = {}\n",
        "    try:\n",
        "        context = {}\n",
        "        exec(code, context)\n",
        "        test_results = context.get(\"test_results\", {})\n",
        "        test_results[\"execution_success\"] = test_results.get(\"execution_success\", 0) + 1\n",
        "    except AssertionError as e:\n",
        "        test_results[\"assertion_failure\"] = test_results.get(\"assertion_failure\", 0) + 1\n",
        "    except Exception as e:\n",
        "        test_results[\"execution_failure\"] = test_results.get(\"execution_failure\", 0) + 1\n",
        "\n",
        "    code_failure = test_results.get(\"code_failure\", 0)\n",
        "    assertion_failure = test_results.get(\"assertion_failure\", 0)\n",
        "    execution_failure = test_results.get(\"execution_failure\", 0)\n",
        "    execution_success = test_results.get(\"execution_success\", 0)\n",
        "    correct = test_results.get(\"correct\", 0)\n",
        "    incorrect = test_results.get(\"incorrect\", 0)\n",
        "    total = (correct + incorrect) or math.inf\n",
        "    accuracy = (1 - code_failure) * (correct / total)\n",
        "\n",
        "    results = dict(\n",
        "        code_failure=code_failure,\n",
        "        execution_success=execution_success,\n",
        "        execution_failure=execution_failure,\n",
        "        assertion_failure=assertion_failure,\n",
        "        correct=correct,\n",
        "        incorrect=incorrect,\n",
        "        accuracy=accuracy,\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_predictions(\n",
        "    df,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    gold_column,\n",
        "    id_labels,\n",
        "    max_length,\n",
        "    dataset_args,\n",
        "    file_path=None,\n",
        "    n=1,\n",
        "    batch_size=4,\n",
        "    num_workers=8,\n",
        "    output_column=\"output\",\n",
        "):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    ns = []\n",
        "    ids = {}\n",
        "\n",
        "    for id_label in id_labels:\n",
        "        ids[id_label] = []\n",
        "\n",
        "    filtered_df = df[df[output_column].isna()] if output_column in df else df # generate predictions only for\n",
        "    unique_df = filtered_df.drop_duplicates(subset=id_labels)\n",
        "\n",
        "    if unique_df.empty:\n",
        "        return df\n",
        "\n",
        "    dataset = ComplexUtteranceCodeDataset(data=unique_df, **dataset_args)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        outs = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            do_sample=n>1,\n",
        "            num_return_sequences=n\n",
        "        )\n",
        "\n",
        "        output = [tokenizer.decode(out, skip_special_tokens=True) for out in outs]\n",
        "        target = [t.strip() for t in list(np.repeat(batch[gold_column], n))]\n",
        "\n",
        "        outputs.extend(output)\n",
        "        targets.extend(target)\n",
        "        ns.extend(list(np.arange(n)) * (batch[\"input_ids\"].shape[0]))\n",
        "        for id_label in id_labels:\n",
        "            ids[id_label].extend(list(np.repeat(batch[id_label], n)))\n",
        "\n",
        "        preds_df = pd.DataFrame({\n",
        "            **{\n",
        "                output_column: outputs,\n",
        "                \"target\": targets,\n",
        "                \"n\": ns,\n",
        "            },\n",
        "            **ids\n",
        "        })\n",
        "\n",
        "        if file_path:\n",
        "            preds_df['sample_id'] = preds_df['sample_id'].astype('int64')\n",
        "            df['sample_id'] = df['sample_id'].astype('int64')\n",
        "\n",
        "            # preds_df = (df.set_index(id_labels)).merge(preds_df, on=id_labels, how='left')\n",
        "\n",
        "            # Merge the DataFrames\n",
        "            suffix_preds = '_preds'\n",
        "            merged_df = pd.merge(df, preds_df, on=id_labels, how='left', suffixes=('', suffix_preds))\n",
        "\n",
        "            # Update 'n' and 'output' in df where they are None\n",
        "            for column in preds_df.columns:\n",
        "              merged_column = f\"{column}{suffix_preds}\"\n",
        "              if merged_column in merged_df:\n",
        "                merged_df[column] = merged_df[column].combine_first(merged_df[merged_column])\n",
        "                merged_df.drop(merged_column, axis=1, inplace=True)\n",
        "            preds_df = merged_df\n",
        "\n",
        "            # preds_df = df.merge(preds_df, on=id_labels, how='left')\n",
        "            preds_df.to_csv(file_path)\n",
        "            # total_preds_count = preds_df['sample_id'].nunique()\n",
        "            # generated_preds_count = preds_df[preds_df[output_column].notna()]['sample_id'].nunique()\n",
        "            # pending_preds_count = preds_df[preds_df[output_column].isna()]['sample_id'].nunique()\n",
        "            # print(f\"Generated {generated_preds_count} / {total_preds_count} ({(100. * generated_preds_count / total_preds_count):.0f}%) and saved to {file_path}\")\n",
        "\n",
        "    return preds_df\n",
        "\n",
        "\n",
        "def humaneval_accuracy_score(\n",
        "    n,\n",
        "    k,\n",
        "    data: pd.DataFrame,\n",
        "    code_column_name: str = \"pred_code\",\n",
        "    score_id_labels1: Union[str, List[str]] = [\"sample_id\", \"n\"],\n",
        "    score_id_labels2: Union[str, List[str]] = \"sample_id\",\n",
        "    score_column_name: str = \"accuracy\",\n",
        "):\n",
        "    test_codes = data.apply(\n",
        "        lambda x: build_test_code(\n",
        "            code=x[code_column_name], imports=x[\"imports\"], test=x[\"test\"]\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "    test_results = test_codes.apply(lambda test_code: eval_code(test_code))\n",
        "    test_results_df = pd.DataFrame.from_records(\n",
        "        test_results.values, index=test_results.index\n",
        "    )\n",
        "    test_scores = (\n",
        "        test_results_df.reset_index(drop=False)\n",
        "        .groupby(score_id_labels1)[score_column_name]\n",
        "        .mean()\n",
        "    )\n",
        "    scores = (\n",
        "        test_scores.reset_index(drop=False)\n",
        "        .groupby(score_id_labels2)[score_column_name]\n",
        "        .max()\n",
        "    )\n",
        "    c = (scores == 1).sum()\n",
        "    print(f\"c = {c}, n= {n}, k = {k}\")\n",
        "    score = pass_at_k(n, c, k)\n",
        "    return dict(score=score, results=test_results_df)\n",
        "\n",
        "\n",
        "def bleu_accuracy_score(\n",
        "    data: pd.DataFrame,\n",
        "    generated_column=\"output\",\n",
        "    gold_column=\"code\",\n",
        "    score_id_labels1: Union[str, List[str]] = [\"sample_id\", \"n\"],\n",
        "    score_id_labels2: Union[str, List[str]] = \"sample_id\",\n",
        "    score_column_name: str = \"bleu_score\",\n",
        "):\n",
        "    eval_results = data.apply(\n",
        "        lambda x: eval_bleu(x[gold_column], x[generated_column]), axis=1\n",
        "    )\n",
        "    eval_results_df = eval_results.to_frame(\"bleu_score\")\n",
        "    test_scores = (\n",
        "        eval_results_df.reset_index(drop=False)\n",
        "        .groupby(score_id_labels1)[score_column_name]\n",
        "        .mean()\n",
        "    )\n",
        "    score = (\n",
        "        test_scores.reset_index(drop=False)\n",
        "        .groupby(score_id_labels2)[score_column_name]\n",
        "        .max()\n",
        "        .mean()\n",
        "    )\n",
        "    return dict(score=score, results=eval_results_df)\n",
        "\n",
        "\n",
        "def model_eval(\n",
        "    n,\n",
        "    k,\n",
        "    results_df=None,\n",
        "    results_file_path=None,\n",
        "    output_column=\"output\",\n",
        "    gold_column=\"code\",\n",
        "    parse_to_code=False,\n",
        "    parse_rules_enabled=False,\n",
        "    compute_humanval=True,\n",
        "    compute_bleu=True,\n",
        "):\n",
        "    results_df = (\n",
        "        pd.read_csv(results_file_path) if results_file_path else results_df.copy()\n",
        "    )\n",
        "    results_df[\"sample_id\"] = results_df[\"sample_id\"].astype(int)\n",
        "    results_df.set_index([\"sample_id\", \"sample_minor_id\", \"n\"], inplace=True)\n",
        "    results_df.sort_index(inplace=True)\n",
        "\n",
        "    code_column = \"generated_code\"\n",
        "    if parse_to_code:\n",
        "        results_df[code_column] = results_df[output_column].apply(\n",
        "            lambda x: parse_code_rep_to_code(x, rules_enabled=parse_rules_enabled)\n",
        "        )\n",
        "    else:\n",
        "        results_df[code_column] = results_df[output_column]\n",
        "\n",
        "    results_df[\"test\"] = results_df[\"test\"].str.replace(\n",
        "        \"= next(iterator)\", \"= next(iterator, None)\", regex=True\n",
        "    )\n",
        "    results_df[code_column] = results_df[code_column].str.replace(\n",
        "        \" = ContentType.\", \" = MessageContentType.\", regex=True\n",
        "    )\n",
        "    results_df[code_column] = results_df[code_column].str.replace(\n",
        "        \"Message.\", \"Messages.\", regex=True\n",
        "    )\n",
        "\n",
        "    humaneval_results = (\n",
        "        humaneval_accuracy_score(n=n, k=k, data=results_df, code_column_name=code_column)\n",
        "        if compute_humanval\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "    bleu_results = (\n",
        "        bleu_accuracy_score(\n",
        "            data=results_df, generated_column=code_column, gold_column=gold_column\n",
        "        )\n",
        "        if compute_bleu\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "    results = dict(\n",
        "        humaneval=humaneval_results,\n",
        "        bleu=bleu_results\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "def pass_at_k(n, c, k):\n",
        "    \"\"\"\n",
        "    :param n: total number of samples\n",
        "    :param c: number of correct samples\n",
        "    :param k: k in pass@$k$\n",
        "    \"\"\"\n",
        "    if (n - c) < k:\n",
        "        return 1.0\n",
        "    score =  1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n",
        "    return score\n",
        "\n",
        "\n",
        "def eval_bleu(code, generated_code):\n",
        "    if not code or not generated_code:\n",
        "        return 0\n",
        "\n",
        "    hypothesis = tokenize_source(code)\n",
        "\n",
        "    try:\n",
        "        reference = tokenize_source(generated_code)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "    n = max(min(len(hypothesis), 4), 1)\n",
        "    weight = 1 / n\n",
        "    weights = (weight,) * n\n",
        "    smoothing_function = SmoothingFunction().method4\n",
        "    score = bleu_score.sentence_bleu(\n",
        "        [reference], hypothesis, weights=weights, smoothing_function=smoothing_function\n",
        "    )\n",
        "    return score\n",
        "\n",
        "\n",
        "def eval_generated_code(\n",
        "    df,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset_args,\n",
        "    target_label,\n",
        "    id_labels,\n",
        "    max_length,\n",
        "    n,\n",
        "    file_path=None,\n",
        "    output_column=\"output\",\n",
        "    gold_column=\"code\",\n",
        "    force_generate_predictions=True,\n",
        "    should_model_eval=True,\n",
        "    batch_size=4\n",
        "):\n",
        "    file_exists = file_path and os.path.exists(file_path)\n",
        "    if file_exists:\n",
        "      print(f\"Loading results from {file_path}\")\n",
        "    preds_df = pd.read_csv(file_path) if file_exists else df\n",
        "\n",
        "    total_preds_count = preds_df['sample_id'].nunique()\n",
        "    pending_preds_count = preds_df[preds_df[output_column].isna()]['sample_id'].nunique() if output_column in preds_df else preds_df['sample_id'].nunique()\n",
        "    generated_preds_count = preds_df[preds_df[output_column].notna()]['sample_id'].nunique() if output_column in preds_df else 0\n",
        "    print(f\"Generated {generated_preds_count} / {total_preds_count} ({(100. * generated_preds_count / total_preds_count):.0f}%)\")\n",
        "\n",
        "    should_generate_predictions = pending_preds_count > 0\n",
        "    if force_generate_predictions or should_generate_predictions:\n",
        "        print(f\"Generating {pending_preds_count} results...\")\n",
        "        preds_df = generate_predictions(\n",
        "            df=preds_df,\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            n=n,\n",
        "            dataset_args=dataset_args,\n",
        "            file_path=file_path,\n",
        "            gold_column=target_label,\n",
        "            id_labels=id_labels,\n",
        "            max_length=max_length,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    if file_path:\n",
        "        print(f\"Loading results from {file_path}\")\n",
        "        results_df = pd.read_csv(file_path)\n",
        "    else:\n",
        "        results_df = preds_df\n",
        "\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY7kNWtAIahm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckyCLXaAIahm"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMjhgO8Iahm",
        "outputId": "3f2b8cd2-8e29-40a3-dccb-c0a18722aa4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape =  (152, 14)\n",
            "test_df (152, 14)\n",
            "test_df (152, 14)\n"
          ]
        }
      ],
      "source": [
        "test_file_path = '/Users/asaf/Workspace/biu/complex-utterance-to-code/build/eval_complex_utterance_to_code_with_intermediate_152_20231112.csv.gz'\n",
        "test_df = load_test_data(test_file_path=test_file_path, id_labels=None)\n",
        "print(\"test_df\", test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fJzYnUiQIahn"
      },
      "outputs": [],
      "source": [
        "def eval_test_data(\n",
        "    pretrained_model_path,\n",
        "    test_df,\n",
        "    model_name,\n",
        "    selected_model_type,\n",
        "    n,\n",
        "    ks = [1, 10],\n",
        "    results_file_path=None,\n",
        "    output_column=\"output\",\n",
        "    gold_column=\"code\",\n",
        "    force_generate_predictions=True,\n",
        "    should_model_eval=True,\n",
        "    batch_size=4,\n",
        "    force=False\n",
        "):\n",
        "    # create a tokenizer and load the model\n",
        "    tokenizer = load_tokenizer(\n",
        "        model_flavour=model_name,\n",
        "        pretrained_model_name_or_path=pretrained_model_names_mapping[model_name]\n",
        "    )\n",
        "    model = load_model(\n",
        "        model_flavour=model_name,\n",
        "        pretrained_model_name_or_path=pretrained_model_path\n",
        "    )\n",
        "\n",
        "    # selected model params\n",
        "    selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
        "    target_label = selected_model_flavour_params.get('target_label')\n",
        "    parse_code = (target_label in ['code_rep', 'code_rep_raw'])\n",
        "    parse_rules_enabled = (target_label == 'code_rep_raw')\n",
        "    slug = selected_model_flavour_params.get('slug')\n",
        "\n",
        "    # load the dataset\n",
        "    dataset_args = get_dataset_args(tokenizer, selected_model_flavour_params)\n",
        "    max_length = dataset_args['max_target_length']\n",
        "\n",
        "    id_labels = ['sample_id'] #['test_id', 'sample_id', 'sample_minor_id']\n",
        "\n",
        "    print(f\"model_id = {model_id}\")\n",
        "    print(f\"slug = {slug}\")\n",
        "    print(f\"n = {n}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    results_df = eval_generated_code(\n",
        "        df=test_df,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_args=dataset_args,\n",
        "        n=n,\n",
        "        target_label=target_label,\n",
        "        id_labels=id_labels,\n",
        "        max_length=max_length,\n",
        "        gold_column=gold_column,\n",
        "        file_path=results_file_path,\n",
        "        force_generate_predictions=force_generate_predictions,\n",
        "        should_model_eval=should_model_eval,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    results = None\n",
        "    if should_model_eval:\n",
        "        for k in ks:\n",
        "            results = model_eval(\n",
        "                n=n,\n",
        "                k=k,\n",
        "                results_df=results_df,\n",
        "                parse_to_code=parse_code,\n",
        "                parse_rules_enabled=parse_rules_enabled,\n",
        "                compute_humanval=True,\n",
        "                compute_bleu=False,\n",
        "                output_column=output_column,\n",
        "                gold_column=gold_column,\n",
        "            )\n",
        "            print(f\"n = {n}, k = {k}\")\n",
        "            print(f\"humaneval = {results['humaneval']['score']}\") if results['humaneval'] else print(\"no pass@k evaluation\")\n",
        "            print(f\"bleu = {results['bleu']['score']}\") if results['bleu'] else print(\"no BLEU evaluation\")\n",
        "            print(f\"\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JbOAP9SGIxY6"
      },
      "outputs": [],
      "source": [
        "models_args = [\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='./experiments/codet5-small-rep2rep-2023-05-23_140209'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Rep2Code, pretrained_model_path='./experiments/codet5-small-rep2code-2023-05-24_122619'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Text2Rep, pretrained_model_path='./experiments/codet5-small-text2rep-2023-05-24_135817'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='./experiments/codet5-small-text2code-2023-05-24_164951'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.RepRaw2RepRaw, pretrained_model_path='./experiments/codet5-small-repraw2repraw-2023-11-08_161837'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Text2RepRaw, pretrained_model_path='./experiments/codet5-small-text2repraw-2023-11-08_214411'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.RepRaw2Code, pretrained_model_path='./experiments/codet5-small-repraw2code-2023-11-08_085740'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.TextRep2Rep, pretrained_model_path='./experiments/codet5-small-text_rep2rep-2023-05-24_171242'),\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.TextRep2Code, pretrained_model_path='./experiments/codet5-small-textrep2code-2023-05-25_045545'),\n",
        "\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='./experiments/codet5-base-rep2rep-2023-05-24_122620'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='./experiments/codet5-base-text2code-2023-05-25_125337'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2Rep, pretrained_model_path='./experiments/codet5-base-text2rep-2023-05-24_143609'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Rep2Code, pretrained_model_path='./experiments/codet5-base-rep2code-2023-05-25_131404'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.RepRaw2RepRaw, pretrained_model_path='./experiments/codet5-base-repraw2repraw-2023-11-08_145043'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2RepRaw, pretrained_model_path='./experiments/codet5-base-text2repraw-2023-11-12_112626'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.RepRaw2Code, pretrained_model_path='./experiments/codet5-base-repraw2code-2023-11-08_214552'),\n",
        "    # dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.TextRep2Rep, pretrained_model_path='./experiments/codet5-base-text_rep2rep-2023-05-26_072118'),\n",
        "\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='./experiments/codet5p-220m-rep2rep-2023-05-24_122638'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.Rep2Code, pretrained_model_path='./experiments/codet5p-220m-rep2code-2023-05-25_110453'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.Text2Rep, pretrained_model_path='./experiments/codet5p-220m-text2rep-2023-05-24_143618'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='./experiments/codet5p-220m-text2code-2023-05-25_111249'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.RepRaw2RepRaw, pretrained_model_path='./experiments/codet5p-220m-repraw2repraw-2023-11-08_132428'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.Text2RepRaw, pretrained_model_path='./experiments/codet5p-220m-text2repraw-2023-11-08_161700'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.RepRaw2Code, pretrained_model_path='./experiments/codet5p-220m-repraw2code-2023-11-08_090909'),\n",
        "    # dict(model_name=Model.CodeT5P220m, selected_model_type=ModelFlavour.TextRep2Rep, pretrained_model_path='./experiments/codet5p-220m-text_rep2rep-2023-05-25_110437'),\n",
        "\n",
        "    dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='/Users/asaf/Downloads/experiments/t5-base-rep2rep-2023-05-25_161415'),\n",
        "    dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.Rep2Code, pretrained_model_path='/Users/asaf/Downloads/experiments/t5-base-rep2code-2023-05-25_160900'),\n",
        "    dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.Text2Rep, pretrained_model_path='/Users/asaf/Downloads/experiments/t5-base-text2rep-2023-05-25_161606'),\n",
        "    dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='/Users/asaf/Downloads/experiments/t5-base-text2code-2023-05-27_171606'),\n",
        "    # dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.RepRaw2RepRaw, pretrained_model_path='./experiments/t5-base-repraw2repraw-2023-11-08_212729'),\n",
        "    # dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.Text2RepRaw, pretrained_model_path='./experiments/t5-base-text2repraw-2023-11-09_092207'),\n",
        "    # dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.RepRaw2Code, pretrained_model_path='./experiments/t5-base-repraw2code-2023-11-12_112659'),\n",
        "    # dict(model_name=Model.T5Base, selected_model_type=ModelFlavour.TextRep2Rep, pretrained_model_path='./experiments/t5-base-textrep2code-2023-05-27_171555'),\n",
        "\n",
        "    # dict(model_name=Model.CodeT5Small, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='./experiments/refit_complex_codet5-small-rep2rep-2023-05-23_031926'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Dji9pUOQHabZ"
      },
      "outputs": [],
      "source": [
        "# test_df = pd.DataFrame({\n",
        "#         \"text\": [\"Check that I received a mail from my advisors or cancel my first meeting with them on Sunday and Monday.\"],\n",
        "#         \"code\": [\"\"],\n",
        "#         \"test_id\": [0],\n",
        "#         \"sample_id\": [0],\n",
        "#         \"sample_minor_id\": [None]\n",
        "# })\n",
        "# test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O_OHe9i9IWcS"
      },
      "outputs": [],
      "source": [
        "# n = 100\n",
        "# args = models_args[0]\n",
        "# # args = dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Text2Code, pretrained_model_path='./experiments/codet5-base-text2code-2023-05-25_125337')\n",
        "# # args = dict(model_name=Model.CodeT5Base, selected_model_type=ModelFlavour.Rep2Rep, pretrained_model_path='./experiments/codet5-base-rep2rep-2023-05-24_122620')\n",
        "# pretrained_model_path = args.get('pretrained_model_path')\n",
        "# selected_model_type = args.get('selected_model_type')\n",
        "# model_name = args.get('model_name')\n",
        "# print(pretrained_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jd9E_gK_ITIS"
      },
      "outputs": [],
      "source": [
        "# # create a tokenizer and load the model\n",
        "# tokenizer = load_tokenizer(\n",
        "#   model_flavour=model_name,\n",
        "#   pretrained_model_name_or_path=pretrained_model_names_mapping[model_name]\n",
        "# )\n",
        "# model = load_model(\n",
        "#   model_flavour=model_name,\n",
        "#   pretrained_model_name_or_path=pretrained_model_path\n",
        "# )\n",
        "\n",
        "# # selected model params\n",
        "# selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
        "# target_label = selected_model_flavour_params.get('target_label')\n",
        "# parse_code = (target_label == 'code_rep')\n",
        "# slug = selected_model_flavour_params.get('slug')\n",
        "\n",
        "# # load the dataset\n",
        "# dataset_args = get_dataset_args(tokenizer, selected_model_flavour_params)\n",
        "# max_length = dataset_args['max_target_length']\n",
        "\n",
        "# BATCH_SIZE = 4\n",
        "# test_dataset = ComplexUtteranceCodeDataset(data=test_df[:BATCH_SIZE], **dataset_args) # remove temp size\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "# model_id = model_name.value\n",
        "# pretrained_model_file = [x for x in pretrained_model_path.split('/') if x][-1]\n",
        "# test_results_file_path = f\"results/test-{str(test_df.shape[0])}-{pretrained_model_file}-n{n}.csv.gz\"\n",
        "# id_labels = ['test_id', 'sample_id', 'sample_minor_id']\n",
        "\n",
        "# print(f\"model_id = {model_id}\")\n",
        "# print(f\"slug = {slug}\")\n",
        "# print(f\"n = {n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "8dd0bb67e2264e57ad346bc2b7b64585",
            "e86207cc8380401c9488fd68305dc07b",
            "464bfca19f8a4432b1880688986164c3",
            "501ad72b8e984a9a9032dcf36fa1baaf",
            "e15946fe593d4fa49a80ee493cc04b9a",
            "3ee68214d1654440b7737a29cc1d59d1",
            "e82ad336f8704096a78d0203c1364cc5",
            "21d8d2eae5404c73ba727ee77eb09296",
            "e568b79739b54d729e7d06055487e551",
            "b3646341a83449fbaa8e3d206d7274c8",
            "f67aef5449ac45e4858c12701ab60642"
          ]
        },
        "id": "pvL1w1IpIahn",
        "outputId": "2668dcad-bf13-4897-ea34-370bd57718a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from /Users/asaf/Downloads/experiments/t5-base-rep2rep-2023-05-25_161415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/asaf/opt/miniconda3/envs/biu/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_id = t5-base\n",
            "slug = rep2rep\n",
            "n = 100\n",
            "\n",
            "Loading results from /Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-152-t5-base-rep2rep-n100.csv.gz\n",
            "Generated 79 / 122 (65%)\n",
            "Generating 43 results...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67196aaf8f654981accf01a108141df5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/43 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading results from /Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-152-t5-base-rep2rep-n100.csv.gz\n",
            "c = 0, n= 100, k = 1\n",
            "n = 100, k = 1\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "c = 0, n= 100, k = 10\n",
            "n = 100, k = 10\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "All runs ended succesfully!\n",
            "\n",
            "Loading model from /Users/asaf/Downloads/experiments/t5-base-rep2code-2023-05-25_160900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/asaf/opt/miniconda3/envs/biu/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_id = t5-base\n",
            "slug = rep2code\n",
            "n = 100\n",
            "\n",
            "Generated 0 / 122 (0%)\n",
            "Generating 122 results...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a973f56a20164fd98f0a10c0c5d28e43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/122 [00:08<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading results from /Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-152-t5-base-rep2code-n100.csv.gz\n",
            "c = 0, n= 100, k = 1\n",
            "n = 100, k = 1\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "c = 0, n= 100, k = 10\n",
            "n = 100, k = 10\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "All runs ended succesfully!\n",
            "\n",
            "Loading model from /Users/asaf/Downloads/experiments/t5-base-text2rep-2023-05-25_161606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/asaf/opt/miniconda3/envs/biu/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_id = t5-base\n",
            "slug = text2rep\n",
            "n = 100\n",
            "\n",
            "Generated 0 / 122 (0%)\n",
            "Generating 122 results...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fbc82085fcc463a98e05a9a477ecd61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/122 [00:08<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading results from /Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-152-t5-base-text2rep-n100.csv.gz\n",
            "c = 0, n= 100, k = 1\n",
            "n = 100, k = 1\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "c = 0, n= 100, k = 10\n",
            "n = 100, k = 10\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "All runs ended succesfully!\n",
            "\n",
            "Loading model from /Users/asaf/Downloads/experiments/t5-base-text2code-2023-05-27_171606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/asaf/opt/miniconda3/envs/biu/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_id = t5-base\n",
            "slug = text2code\n",
            "n = 100\n",
            "\n",
            "Generated 0 / 122 (0%)\n",
            "Generating 122 results...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e1430a39fa5424894e77be0e2691483",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/122 [00:08<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading results from /Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-152-t5-base-text2code-n100.csv.gz\n",
            "c = 0, n= 100, k = 1\n",
            "n = 100, k = 1\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "c = 0, n= 100, k = 10\n",
            "n = 100, k = 10\n",
            "humaneval = 0.0\n",
            "no BLEU evaluation\n",
            "\n",
            "All runs ended succesfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for args in models_args:\n",
        "  pretrained_model_path = args.get('pretrained_model_path')\n",
        "  selected_model_type = args.get('selected_model_type')\n",
        "  model_name = args.get('model_name')\n",
        "  print(f\"Loading model from {pretrained_model_path}\")\n",
        "\n",
        "  n = 100\n",
        "  selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
        "  slug = selected_model_flavour_params.get('slug')\n",
        "  model_id = model_name.value\n",
        "  test_results_file_path = f\"/Users/asaf/Workspace/biu/complex-utterance-to-code/dist/experiments_results/test-{str(test_df.shape[0])}-{model_id}-{slug}-n{n}.csv.gz\"\n",
        "\n",
        "  results = eval_test_data(\n",
        "      pretrained_model_path=pretrained_model_path,\n",
        "      test_df=test_df,\n",
        "      model_name=model_name,\n",
        "      selected_model_type=selected_model_type,\n",
        "      n=n,\n",
        "      ks=[1, 10],\n",
        "      results_file_path=test_results_file_path,\n",
        "      force_generate_predictions=False,\n",
        "      should_model_eval=True,\n",
        "      batch_size=1\n",
        "  )\n",
        "\n",
        "  # print(results['humaneval'])\n",
        "  print(f\"All runs ended succesfully!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWbQ7Tdo4iCd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('results/test-152-codet5-small-repraw2code-n100.csv.gz', compression='gzip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siZT2BmOAIT6"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckTTpFBUmrFc"
      },
      "outputs": [],
      "source": [
        "n = 2\n",
        "# create a tokenizer and load the model\n",
        "tokenizer = load_tokenizer(\n",
        "  model_flavour=model_name,\n",
        "  pretrained_model_name_or_path=pretrained_model_names_mapping[model_name]\n",
        ")\n",
        "model = load_model(\n",
        "  model_flavour=model_name,\n",
        "  pretrained_model_name_or_path=pretrained_model_path\n",
        ")\n",
        "\n",
        "# selected model params\n",
        "selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
        "target_label = selected_model_flavour_params.get('target_label')\n",
        "parse_code = (target_label == 'code_rep')\n",
        "slug = selected_model_flavour_params.get('slug')\n",
        "\n",
        "# load the dataset\n",
        "dataset_args = get_dataset_args(tokenizer, selected_model_flavour_params)\n",
        "max_length = dataset_args['max_target_length']\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "test_dataset = ComplexUtteranceCodeDataset(data=test_df[:BATCH_SIZE], **dataset_args) # remove temp size\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "model_id = model_name.value\n",
        "pretrained_model_file = [x for x in pretrained_model_path.split('/') if x][-1]\n",
        "test_results_file_path = f\"results/test-{str(test_df.shape[0])}-{pretrained_model_file}-n{n}.csv.gz\"\n",
        "id_labels = ['test_id', 'sample_id', 'sample_minor_id']\n",
        "\n",
        "print(f\"model_id = {model_id}\")\n",
        "print(f\"slug = {slug}\")\n",
        "print(f\"n = {n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ruOq1LIPRG"
      },
      "outputs": [],
      "source": [
        "dataloader = test_dataloader\n",
        "gold_column = target_label\n",
        "\n",
        "model.eval()\n",
        "outputs = []\n",
        "targets = []\n",
        "ns = []\n",
        "ids = {}\n",
        "for id_label in id_labels:\n",
        "    ids[id_label] = []\n",
        "\n",
        "for batch in tqdm(dataloader):\n",
        "    outs = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_length=max_length,\n",
        "        do_sample=(n>1),\n",
        "        num_return_sequences=2\n",
        "    )\n",
        "\n",
        "    output = [tokenizer.decode(out, skip_special_tokens=True) for out in outs]\n",
        "    target = [t.strip() for t in list(np.repeat(batch[gold_column], n))]\n",
        "\n",
        "    outputs.extend(output)\n",
        "    targets.extend(target)\n",
        "    ns.extend(list(np.arange(n)) * (batch[\"input_ids\"].shape[0]))\n",
        "    for id_label in id_labels:\n",
        "        ids[id_label].extend(list(np.repeat(batch[id_label], n)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPAZe7BIPdpV"
      },
      "outputs": [],
      "source": [
        "model_name=Model.CodeT5P220m\n",
        "selected_model_type=ModelFlavour.Rep2Rep\n",
        "# selected_model_type=ModelFlavour.Text2Code\n",
        "pretrained_model_path='./experiments/codet5p-220m-rep2rep-2023-05-24_122638'\n",
        "# pretrained_model_path = './experiments/codet5p-220m-text2code-2023-05-25_111249'\n",
        "k = 10\n",
        "test_df = test_df\n",
        "\n",
        "# create a tokenizer and load the model\n",
        "tokenizer = load_tokenizer(\n",
        "  model_flavour=model_name,\n",
        "  pretrained_model_name_or_path=pretrained_model_names_mapping[model_name]\n",
        ")\n",
        "model = load_model(\n",
        "  model_flavour=model_name,\n",
        "  pretrained_model_name_or_path=pretrained_model_path\n",
        ")\n",
        "\n",
        "# selected model params\n",
        "selected_model_flavour_params = model_flavour_params[selected_model_type]\n",
        "target_label = selected_model_flavour_params.get('target_label')\n",
        "parse_code = (target_label == 'code_rep')\n",
        "slug = selected_model_flavour_params.get('slug')\n",
        "\n",
        "# load the dataset\n",
        "dataset_args = get_dataset_args(tokenizer, selected_model_flavour_params)\n",
        "max_length = dataset_args['max_target_length']\n",
        "\n",
        "test_dataset = ComplexUtteranceCodeDataset(data=test_df, **dataset_args)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=2)\n",
        "\n",
        "model_id = model_name.value\n",
        "pretrained_model_file = [x for x in pretrained_model_path.split('/') if x][-1]\n",
        "test_results_file_path = f\"results/test-{str(test_df.shape[0])}-{pretrained_model_file}-k{k}.csv.gz\"\n",
        "id_labels = ['test_id', 'sample_id', 'sample_minor_id']\n",
        "\n",
        "print(f\"model_id = {model_id}\")\n",
        "print(f\"slug = {slug}\")\n",
        "print(f\"k = {k}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31pj-hO1VkBs"
      },
      "outputs": [],
      "source": [
        "results_file_path = test_results_file_path\n",
        "compute_humanval = True\n",
        "compute_bleu = False\n",
        "output_column = \"output\"\n",
        "gold_column = target_label\n",
        "\n",
        "results_df = pd.read_csv(results_file_path)\n",
        "results_df[\"sample_id\"] = results_df[\"sample_id\"].astype(int)\n",
        "results_df.set_index([\"sample_id\", \"sample_minor_id\", \"n\"], inplace=True)\n",
        "results_df.sort_index(inplace=True)\n",
        "\n",
        "code_column = \"generated_code\"\n",
        "if parse_code:\n",
        "    results_df[code_column] = results_df[output_column].apply(\n",
        "        lambda x: parse_code_rep_to_code(x)\n",
        "    )\n",
        "else:\n",
        "    results_df[code_column] = results_df[output_column]\n",
        "\n",
        "results_df[\"test\"] = results_df[\"test\"].str.replace(\n",
        "    \"= next(iterator)\", \"= next(iterator, None)\", regex=True\n",
        ")\n",
        "results_df[code_column] = results_df[code_column].str.replace(\n",
        "    \" = ContentType.\", \" = MessageContentType.\", regex=True\n",
        ")\n",
        "results_df[code_column] = results_df[code_column].str.replace(\n",
        "    \"Message.\", \"Messages.\", regex=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrBm0EtJsUm2"
      },
      "outputs": [],
      "source": [
        "data = results_df\n",
        "code_column_nam = \"pred_code\"\n",
        "score_id_labels = [\"sample_id\"]\n",
        "score_column_name = \"accuracy\"\n",
        "code_column_name = code_column\n",
        "\n",
        "test_codes = data.apply(\n",
        "    lambda x: build_test_code(\n",
        "        code=x[code_column_name], imports=x[\"imports\"], test=x[\"test\"]\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "test_results = test_codes.apply(lambda test_code: eval_code(test_code))\n",
        "test_results_df = pd.DataFrame.from_records(\n",
        "    test_results.values, index=test_results.index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A97A0WE-suMS"
      },
      "outputs": [],
      "source": [
        "test_results_df2 = test_results_df.reset_index(drop=False).groupby([\"sample_id\", \"n\"])[score_column_name].mean()\n",
        "test_results_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9y61CdYOryL"
      },
      "outputs": [],
      "source": [
        "test_results_df2.reset_index(drop=False).groupby([\"sample_id\"])[score_column_name].max().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixtbnP--I83F"
      },
      "outputs": [],
      "source": [
        "test_results_df.reset_index(inplace=True)\n",
        "test_results_df['sample_minor_id'].fillna('a', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGfAevNvGkyY"
      },
      "outputs": [],
      "source": [
        "test_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-doKGmzHcmG"
      },
      "outputs": [],
      "source": [
        "test_results_df3 = test_results_df.loc[test_results_df.groupby(['sample_id', \"sample_minor_id\"])[score_column_name].idxmax()]\n",
        "test_results_df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j79CPu3fKJiG"
      },
      "outputs": [],
      "source": [
        "test_results_df3[['code_failure', 'execution_success', 'execution_failure', 'assertion_failure', 'correct', 'incorrect']].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R77K9XEtLhlu"
      },
      "outputs": [],
      "source": [
        "test_results_df3[test_results_df3['accuracy'] == 1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv_gIfCSSzLn"
      },
      "outputs": [],
      "source": [
        "task_df = pd.read_csv('./data/task_oriented_complex_utterances.csv')\n",
        "task_df.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)\n",
        "task_df.set_index(['ID'], inplace=True)\n",
        "task_df.sort_index(inplace=True)\n",
        "task_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okVs59NTatMy"
      },
      "outputs": [],
      "source": [
        "cflow_df = task_df[['Sequence', 'Condition', 'Loop', 'Composition']].fillna(0)\n",
        "\n",
        "cflow_df[cflow_df['Sequence'] > 0]['Sequence'] = 1.0\n",
        "cflow_df[cflow_df['Condition'] > 0]['Condition'] = 1.0\n",
        "cflow_df[cflow_df['Loop'] > 0]['Loop'] = 1.0\n",
        "cflow_df[cflow_df['Composition'] > 0]['Composition'] = 1.0\n",
        "\n",
        "test_results_df4 = test_results_df3.set_index(['sample_id']).join(cflow_df)\n",
        "test_results_df4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6meReAReb7k9"
      },
      "outputs": [],
      "source": [
        "test_results_df4[['Sequence', 'Condition', 'Loop', 'Composition']].multiply(test_results_df4['accuracy'], axis=0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV1b9dVacN9v"
      },
      "outputs": [],
      "source": [
        "test_results_df4[['Sequence', 'Condition', 'Loop', 'Composition']].multiply(test_results_df4['accuracy'], axis=0).sum() / test_results_df4[['Sequence', 'Condition', 'Loop', 'Composition']].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDVHnpDpNN_7"
      },
      "outputs": [],
      "source": [
        "test_results_df4['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAHb2XgINows"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3],\n",
        "    'B': [4, 5, 6],\n",
        "    'C': [7, 8, 9]\n",
        "})\n",
        "\n",
        "# Creating a Series\n",
        "s = pd.Series([10, 20, 30])\n",
        "\n",
        "# Multiply the DataFrame by the Series\n",
        "result = df.multiply(s, axis=0)  # 'axis=0' specifies row-wise operation\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8higFRh6qM4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrames\n",
        "# df1 and df2 should have the same columns for x-axis and y-axis\n",
        "# Example:\n",
        "models = ['CodeT5Small',\t'CodeT5Base',\t'CodeT5+',\t'T5']\n",
        "methods = ['Baseline', '+langRep', '+codeRep', '+lang+codeRep']\n",
        "df_k1 = pd.DataFrame({\n",
        "    'x':methods,\n",
        "    'CodeT5Small': [22.92, 19.70, 16.45, 19.46],\n",
        "    'CodeT5Base': [16.31, 19.73, 18.16, 20.25],\n",
        "    'CodeT5+': [21.76, 18.21, 21.47, 19.40],\n",
        "    'T5': [0, 0, 15.79, 22.94],\n",
        "})\n",
        "df_k10 = pd.DataFrame({\n",
        "    'x': methods,\n",
        "    'CodeT5Small': [21.63, 19.27, 21.07, 24.54],\n",
        "    'CodeT5Base': [16.17, 18.15, 23.95, 23.61],\n",
        "    'CodeT5+': [22.87, 19.89, 18.54, 29.82],\n",
        "    'T5': [0, 0, 28.08, 26.86],\n",
        "})\n",
        "\n",
        "# Create a figure and a set of subplots (in this case, just one)\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for model_name in models:\n",
        "    # Plot the first DataFrame\n",
        "    ax.plot(df_k1['x'], df_k1[model_name], label=f'{model_name} (pass@1)')\n",
        "\n",
        "for model_name in models:\n",
        "    # Plot the second DataFrame\n",
        "    ax.plot(df_k10['x'], df_k10[model_name], label=f'{model_name} (pass@10)')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_ylabel('Pass Rate')\n",
        "ax.set_title('Ablations Results')\n",
        "\n",
        "# Add a legend\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6IPfFedchWk"
      },
      "outputs": [],
      "source": [
        "x_values = ['CodeT5Small', 'CodeT5Base', 'CodeT5+', 'T5', 'GPT3.5', 'GPT4']\n",
        "\n",
        "baslines_k1 = [22.92, 16.31, 21.76, 0, 4.91, 5.74]\n",
        "baslines_k10 = [21.63, 16.17, 22.87, 0, 17.62, 0]\n",
        "\n",
        "lang_rep_k1 = [19.70, 19.73, 18.21, 0, np.nan, np.nan]\n",
        "lang_rep_k10 = [19.27, 18.15, 19.89, 0, np.nan, np.nan]\n",
        "\n",
        "code_rep_k1 = [16.45, 18.16, 21.47, 15.79, np.nan, np.nan]\n",
        "code_rep_k10 = [21.07, 23.95, 18.54, 28.08, np.nan, np.nan]\n",
        "\n",
        "intermediate_k1 = [19.46, 20.25, 19.40, 22.94, np.nan, np.nan]\n",
        "intermediate_k10 = [24.54, 23.61, 29.82, 26.86, np.nan, np.nan]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEW3WGdQlbF2"
      },
      "outputs": [],
      "source": [
        "df_k1 = pd.DataFrame({\n",
        "    'Models': x_values,\n",
        "    'Baseline': baslines_k1,\n",
        "    'LangRep': lang_rep_k1,\n",
        "    'CodeRep': code_rep_k1,\n",
        "    'Rep2Rep': intermediate_k1\n",
        "})\n",
        "\n",
        "df_k10 = pd.DataFrame({\n",
        "    'Models': x_values,\n",
        "    'Baseline': baslines_k10,\n",
        "    'LangRep': lang_rep_k10,\n",
        "    'CodeRep': code_rep_k10,\n",
        "    'Rep2Rep': intermediate_k10\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rrl2kzZ6qQh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BYShhLz6qRw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOtGO5J96qVQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGYqOz5ko_uC"
      },
      "outputs": [],
      "source": [
        "df_k1.fillna(df_k1.mean()).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_XVvsCPp8VR"
      },
      "outputs": [],
      "source": [
        "df_k10.fillna(df_k10.mean()).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xISQvLbdrzvC"
      },
      "outputs": [],
      "source": [
        "df_k10.fillna(df_k10.mean()).mean().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaE0NYHgoRKx"
      },
      "outputs": [],
      "source": [
        "dfk1 = df_k1.T[1:]\n",
        "dfk1.columns = df_k1['Models'].to_list()\n",
        "dfk1_ = dfk1[['CodeT5Small',\t'CodeT5Base',\t'CodeT5+',\t'T5']]\n",
        "dfk1_.rename(index={'Baseline': 'Baseline', 'LangRep': '+langRep', 'CodeRep': '+codeRep', 'Rep2Rep': '+lang+codeRep'}, inplace=True)\n",
        "dfk1_.plot(legend=True, ylabel=\"Pass Rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEDR2KO4pDPm"
      },
      "outputs": [],
      "source": [
        "dfk10 = df_k10.T[1:]\n",
        "dfk10.columns = df_k10['Models'].to_list()\n",
        "dfk10_ = dfk10[['CodeT5Small',\t'CodeT5Base',\t'CodeT5+',\t'T5']]\n",
        "dfk10_.rename(index={'Baseline': 'Baseline', 'LangRep': '+langRep', 'CodeRep': '+codeRep', 'Rep2Rep': '+lang+codeRep'}, inplace=True)\n",
        "dfk10_.plot(legend=True, ylabel=\"Pass Rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssbdkT3G0sz-"
      },
      "outputs": [],
      "source": [
        "pd.concat([dfk1_, dfk10_], axis=1).plot(legend=True, ylabel=\"Pass Rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlJEHmlHrqJ4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_values = ['Baseline', '+langRep', '+codeRep', '+langRep+codeRep']\n",
        "y1_values = df_k1[:4]#.fillna(df_k1.mean()).mean()\n",
        "y2_values = df_k10[:4]#.fillna(df_k10.mean()).mean()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot data\n",
        "for column in y1_values.columns:\n",
        "  ax.plot(x_values, y1_values[column], label='pass@1')\n",
        "  ax.plot(x_values, y2_values[column], label='pass@10')\n",
        "\n",
        "# Add some space around the plot within the bounding box\n",
        "ax.margins(0.15)\n",
        "\n",
        "plt.title('Ablation Results')\n",
        "\n",
        "plt.ylabel('Pass rate')  # Labels y-axis as 'Y Values'\n",
        "\n",
        "# Annotate the lines\n",
        "# plt.annotate('pass@1', (x_values[-1], y1_values[-1]), textcoords=\"offset points\", xytext=(-10,-10))\n",
        "# plt.annotate('pass@10', (x_values[-1], y2_values[-1]), textcoords=\"offset points\", xytext=(-10,10))\n",
        "\n",
        "# Add major gridlines in the y direction in light gray\n",
        "# ax.yaxis.grid(True, which='major', color='lightgray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Add minor gridlines in the y direction in light gray\n",
        "# ax.yaxis.grid(True, which='minor', color='lightgray', linestyle=':', linewidth=0.5)\n",
        "\n",
        "ax.set_ylabel('Pass rate')\n",
        "ax.legend(loc='best')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# plt.savefig('./reports/ablation-results.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJP66XbWm6QT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDNs5ucRoqVS"
      },
      "outputs": [],
      "source": [
        "# Fill missing values using interpolation\n",
        "df['Baseline'].interpolate(method='linear', inplace=True)\n",
        "df['Baseline'].interpolate(method='linear', inplace=True)\n",
        "df['Baseline'].interpolate(method='linear', inplace=True)\n",
        "df['Baseline'].interpolate(method='linear', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELwu2AWFlSNZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(x_values, sorted(baslines_k1))  # Python uses the indices as x-values\n",
        "plt.plot(x_values, sorted(lang_rep_k1))  # Python uses the indices as x-values\n",
        "plt.plot(x_values, sorted(code_rep_k1))  # Python uses the indices as x-values\n",
        "plt.plot(x_values, sorted(intermediate_k1))  # Python uses the indices as x-values\n",
        "# plt.xlabel('Index')  # Labels x-axis as 'Index'\n",
        "plt.ylabel('Y Values')  # Labels y-axis as 'Y Values'\n",
        "plt.title('Line Chart')  # Title for the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUSJ09-GlVUs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21d8d2eae5404c73ba727ee77eb09296": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee68214d1654440b7737a29cc1d59d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464bfca19f8a4432b1880688986164c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d8d2eae5404c73ba727ee77eb09296",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e568b79739b54d729e7d06055487e551",
            "value": 0
          }
        },
        "501ad72b8e984a9a9032dcf36fa1baaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3646341a83449fbaa8e3d206d7274c8",
            "placeholder": "",
            "style": "IPY_MODEL_f67aef5449ac45e4858c12701ab60642",
            "value": " 0/43 [00:01&lt;?, ?it/s]"
          }
        },
        "8dd0bb67e2264e57ad346bc2b7b64585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e86207cc8380401c9488fd68305dc07b",
              "IPY_MODEL_464bfca19f8a4432b1880688986164c3",
              "IPY_MODEL_501ad72b8e984a9a9032dcf36fa1baaf"
            ],
            "layout": "IPY_MODEL_e15946fe593d4fa49a80ee493cc04b9a"
          }
        },
        "b3646341a83449fbaa8e3d206d7274c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e15946fe593d4fa49a80ee493cc04b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e568b79739b54d729e7d06055487e551": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e82ad336f8704096a78d0203c1364cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86207cc8380401c9488fd68305dc07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ee68214d1654440b7737a29cc1d59d1",
            "placeholder": "",
            "style": "IPY_MODEL_e82ad336f8704096a78d0203c1364cc5",
            "value": "  0%"
          }
        },
        "f67aef5449ac45e4858c12701ab60642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
