{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "WORK_AREA = '..'\n",
    "os.chdir(WORK_AREA)\n",
    "\n",
    "paths = ['./src/', './src/api/v6']\n",
    "for path in paths:\n",
    "    path = os.path.normcase(path)\n",
    "    if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import openai\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2TokenizerFast\n",
    "import math\n",
    "import tokenize\n",
    "from nltk.translate import bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_API_ORG\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build prompt from the API docstrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_spec_prompt():\n",
    "    prompt_dict = {}\n",
    "    for prompt_file in glob.glob('./config/prompts/**/*.txt'):\n",
    "        key = os.path.basename(prompt_file).split('.')[0].lower()\n",
    "        with open(prompt_file, \"r\") as f:\n",
    "            prompt_dict[key] = f.read()\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for key, value in prompt_dict.items():\n",
    "        prompt = prompt + f\"# {key.upper()}:\\n\\n{value}\\n\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17187"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = build_spec_prompt()\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(prompt, max_length=51200, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples prompt**\n",
    "\n",
    "\n",
    "Building a prompt from the text and code examples generated by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'build/train_complex_utterance_to_code_with_intermediate_40k.csv.gz'\n",
    "examples_df = pd.read_csv(file_path)\n",
    "examples_df = examples_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example_prompt(text, code=None):\n",
    "    examples_prompt = f\"text: \\n{text}\\n\\n\"\n",
    "    examples_prompt += f\"code: \\n{code}\\n\\n\\n\" if code else f\"code: \\n\"\n",
    "    \n",
    "    return examples_prompt\n",
    "\n",
    "\n",
    "def build_examples_prompt(examples_prompt, df, limit=10):\n",
    "    examples_prompt = examples_prompt or \"\"\n",
    "    for index, row in df[:limit].iterrows():\n",
    "        examples_prompt += build_example_prompt(text=row['text'], code=row['code'])\n",
    "    \n",
    "    return examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3420"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Transform text to Python code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "prompt = build_examples_prompt(examples_prompt, examples_df, limit=18)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(prompt, max_length=51200, truncation=True)[\"input_ids\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'eval_complex_utterance_to_code_with_intermediate_82_20230519.csv.gz'\n",
    "base_path = '/Users/asaf/Workspace/biu/complex-utterance-to-code/build'\n",
    "eval_df = pd.read_csv(os.path.join(base_path, file_name))\n",
    "eval_df = eval_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"code\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_code(code: str, imports: str, test: str, code_embed_str: str = '# end code block to test', fail_on_error: bool = False, verbose: str = 'Fatal'):\n",
    "  try:\n",
    "    code_insert_idx = test.find(code_embed_str)\n",
    "    program_code = imports\n",
    "    program_code += '\\n'\n",
    "    program_code += test[:code_insert_idx]\n",
    "    program_code += code\n",
    "    program_code += '\\n'\n",
    "    program_code += test[code_insert_idx:]\n",
    "  except Exception as e:\n",
    "    if verbose == 'Error':\n",
    "      print('[ERROR] Failed to unparse code rep to code\\n', e)\n",
    "    if fail_on_error:\n",
    "      raise e\n",
    "    program_code = ''\n",
    "  finally:\n",
    "    return program_code\n",
    "  \n",
    "  \n",
    "def tokenize_source(code):\n",
    "    file_path = \"/tmp/example.py\"\n",
    "\n",
    "    with open(file_path, \"w\") as text_file:\n",
    "        text_file.write(code)\n",
    "        \n",
    "    with open(file_path, 'rb') as f:\n",
    "        tokens_gen = tokenize.tokenize(f.readline)\n",
    "        tokens = []\n",
    "        try:\n",
    "          for token in tokens_gen:\n",
    "            tokens.append(token.string)\n",
    "        except Exception as e:\n",
    "          pass\n",
    "        \n",
    "    os.remove(file_path)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def eval_code(code: str):\n",
    "  test_results = {}\n",
    "  try:\n",
    "    context = {}\n",
    "    exec(code, context)\n",
    "    test_results = context.get('test_results', {})\n",
    "  except AssertionError as e:\n",
    "    test_results['test_failuers'] = test_results.get('test_failuers', 0) + 1\n",
    "  except Exception as e:\n",
    "    test_results['code_failure'] = test_results.get('code_failure', 0) + 1\n",
    "\n",
    "  code_failure = test_results.get('code_failure', 0)\n",
    "  correct = test_results.get('correct', 0)\n",
    "  incorrect = test_results.get('incorrect', 0)\n",
    "  total = (correct + incorrect) or math.inf\n",
    "  accuracy = (1 - code_failure) * (correct / total)\n",
    "\n",
    "  results = dict(\n",
    "    code_failure = code_failure,\n",
    "    correct = correct,\n",
    "    incorrect = incorrect,\n",
    "    accuracy = accuracy,\n",
    "  )\n",
    "\n",
    "  return results\n",
    "\n",
    "\n",
    "def eval_bleu(code, generated_code):\n",
    "  hypothesis = tokenize_source(code)\n",
    "  reference = tokenize_source(generated_code)\n",
    "  weights = (0.25, 0.25, 0.25, 0.25)\n",
    "  score = bleu_score.sentence_bleu([reference], hypothesis, weights=weights)\n",
    "  return score\n",
    "\n",
    "\n",
    "def humaneval_accuracy_score(\n",
    "    data: pd.DataFrame, \n",
    "    code_column_name: str = 'pred_code', \n",
    "    score_id_labels: Union[str, List[str]] = 'sample_id', \n",
    "    score_column_name: str = 'accuracy', \n",
    "):\n",
    "    test_codes = data.apply(lambda x: build_test_code(code=x[code_column_name], imports=x['imports'], test=x['test']), axis=1)\n",
    "    test_results = test_codes.apply(lambda test_code: eval_code(test_code))\n",
    "    test_results_df = pd.DataFrame.from_records(\n",
    "        test_results.values, index=test_results.index\n",
    "    )\n",
    "    score = test_results_df.reset_index(drop=False).groupby(score_id_labels)[score_column_name].mean().mean()\n",
    "    return dict(score=score, results=test_results_df)\n",
    "\n",
    "\n",
    "def bleu_accuracy_score(\n",
    "    data: pd.DataFrame, \n",
    "    generated_column='output', \n",
    "    gold_column='code',\n",
    "    score_id_labels: Union[str, List[str]] = 'sample_id', \n",
    "    score_column_name: str = 'bleu_score', \n",
    "):\n",
    "    eval_results = data.apply(lambda x: eval_bleu(x[gold_column], x[generated_column]), axis=1)\n",
    "    eval_results_df = eval_results.to_frame('bleu_score')\n",
    "    score = eval_results_df.reset_index(drop=False).groupby(score_id_labels)[score_column_name].mean().mean()\n",
    "    return dict(score=score, results=eval_results_df)\n",
    "  \n",
    "  \n",
    "def model_eval(\n",
    "    results_file_path, \n",
    "    output_column='output', \n",
    "    gold_column='code', \n",
    "    parse_to_code=False, \n",
    "    compute_humanval=True, \n",
    "    compute_bleu=True\n",
    "):\n",
    "    results_df = pd.read_csv(results_file_path, compression='gzip')\n",
    "    \n",
    "    results_df['sample_id'] = results_df['sample_id'].astype(int)\n",
    "    results_df.set_index(['sample_id', 'sample_minor_id'], inplace=True)\n",
    "    results_df.sort_index(inplace=True)\n",
    "    \n",
    "    code_column = 'generated_code'\n",
    "    if parse_to_code:\n",
    "        results_df[code_column] = results_df[output_column].apply(lambda x: parse_code_rep_to_code(x))\n",
    "    else: \n",
    "        results_df[code_column] = results_df[output_column]\n",
    "        \n",
    "    results_df['test'] = results_df['test'].str.replace(\"= next(iterator)\", \"= next(iterator, None)\")\n",
    "    results_df[code_column] = results_df[code_column].str.replace(\" = ContentType.\", \" = MessageContentType.\")\n",
    "    results_df[code_column] = results_df[code_column].str.replace(\"Message.\", \"Messages.\")\n",
    "\n",
    "    humaneval_results = humaneval_accuracy_score(\n",
    "        data=results_df, \n",
    "        code_column_name=code_column) if compute_humanval else None\n",
    "    \n",
    "    bleu_results = bleu_accuracy_score(\n",
    "        data=results_df, \n",
    "        generated_column=code_column, \n",
    "        gold_column=gold_column) if compute_bleu else None\n",
    "    \n",
    "    results = dict(\n",
    "        humaneval = humaneval_results,\n",
    "        bleu = bleu_results\n",
    "    )\n",
    "    return results\n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whisper-1', 'babbage', 'gpt-3.5-turbo', 'davinci', 'text-davinci-edit-001', 'text-davinci-003', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'babbage-code-search-text', 'babbage-similarity', 'code-search-babbage-text-001', 'text-curie-001', 'gpt-4', 'code-search-babbage-code-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'curie-instruct-beta', 'gpt-4-0314', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'gpt-3.5-turbo-0301', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n"
     ]
    }
   ],
   "source": [
    "oai_models = openai.Model.list()\n",
    "print([model_data['id'] for model_data in oai_models['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage-code-search-code', 'code-davinci-edit-001', 'babbage-code-search-text', 'code-search-babbage-text-001', 'code-search-babbage-code-001', 'ada-code-search-code', 'code-search-ada-text-001', 'ada-code-search-text', 'code-search-ada-code-001']\n"
     ]
    }
   ],
   "source": [
    "print([model_data['id'] for model_data in oai_models['data'] if 'code' in model_data['id']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0016a729c384d1c9b51f18470f72f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = build_examples_prompt(examples_prompt, examples_df, limit=15)\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm_notebook(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./build/openai-text-davinci-003-eval_complex_utterance_to_code_with_intermediate_82_20230519.csv.gz'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_file_path = f'./build/openai-{MODEL_NAME}-{file_name}'\n",
    "responses_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sample_minor_id</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>imports</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check the availability of Pepsi at Walmart and...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Check ] ]...</td>\n",
       "      <td>[ Module [ product_name = ProductName.resolve_...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_b</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Play the new Taylor Swift album and pull up my...</td>\n",
       "      <td>album = Album.resolve_from_text(\"the new Taylo...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Play ] ] ...</td>\n",
       "      <td>[ Module [ album = Album.resolve_from_text('th...</td>\n",
       "      <td>album_name = AlbumName.resolve_from_text(\"the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3_a</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Send a message to dad if it rains tomorrow.</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Body [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>destination = Contact.resolve_from_text(\"dad\")...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index test_id  sample_id sample_minor_id   \n",
       "0      0       0          0             NaN  \\\n",
       "1      1     1_a          1               a   \n",
       "2      2     1_b          1               b   \n",
       "3      3       2          2             NaN   \n",
       "4      4     3_a          3               a   \n",
       "\n",
       "                                                text   \n",
       "0  Check the availability of Pepsi at Walmart and...  \\\n",
       "1  If it's raining tomorrow morning, set my alarm...   \n",
       "2  If it's raining tomorrow morning, set my alarm...   \n",
       "3  Play the new Taylor Swift album and pull up my...   \n",
       "4        Send a message to dad if it rains tomorrow.   \n",
       "\n",
       "                                                code   \n",
       "0  product_name = ProductName.resolve_from_text(\"...  \\\n",
       "1  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "3  album = Album.resolve_from_text(\"the new Taylo...   \n",
       "4  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "\n",
       "                                                test   \n",
       "0  # test data\\ndata_model = DataModel(reset=True...  \\\n",
       "1  # test data\\ndata_model = DataModel(reset=True...   \n",
       "2  # test data\\ndata_model = DataModel(reset=True...   \n",
       "3  # test data\\ndata_model = DataModel(reset=True...   \n",
       "4  # test data\\ndata_model = DataModel(reset=True...   \n",
       "\n",
       "                                             imports   \n",
       "0  from entities.generic import *\\nfrom entities....  \\\n",
       "1  from entities.generic import *\\nfrom entities....   \n",
       "2  from entities.generic import *\\nfrom entities....   \n",
       "3  from entities.generic import *\\nfrom entities....   \n",
       "4  from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                                            lang_rep   \n",
       "0  [ root [ S [ Command [ Action [ hd [ Check ] ]...  \\\n",
       "1  [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "2  [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "3  [ root [ S [ Command [ Action [ hd [ Play ] ] ...   \n",
       "4  [ root [ S [ Command [ Condition [ If [ Body [...   \n",
       "\n",
       "                                            code_rep   \n",
       "0  [ Module [ product_name = ProductName.resolve_...  \\\n",
       "1  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "2  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "3  [ Module [ album = Album.resolve_from_text('th...   \n",
       "4  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "\n",
       "                                              output  \n",
       "0  product_name = ProductName.resolve_from_text(\"...  \n",
       "1  date_time = DateTime.resolve_from_text(\"tomorr...  \n",
       "2  date_time = DateTime.resolve_from_text(\"tomorr...  \n",
       "3  album_name = AlbumName.resolve_from_text(\"the ...  \n",
       "4  destination = Contact.resolve_from_text(\"dad\")...  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_data = [response['choices'][0]['text'] for response in responses]\n",
    "eval_oai_df = eval_df.copy()\n",
    "eval_oai_df['output'] = pd.Series(responses_data)\n",
    "eval_oai_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_oai_df.to_csv(responses_file_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sample_minor_id</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>imports</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check the availability of Pepsi at Walmart and...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Check ] ]...</td>\n",
       "      <td>[ Module [ product_name = ProductName.resolve_...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_b</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Play the new Taylor Swift album and pull up my...</td>\n",
       "      <td>album = Album.resolve_from_text(\"the new Taylo...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Play ] ] ...</td>\n",
       "      <td>[ Module [ album = Album.resolve_from_text('th...</td>\n",
       "      <td>album_name = AlbumName.resolve_from_text(\"the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3_a</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Send a message to dad if it rains tomorrow.</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Body [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>destination = Contact.resolve_from_text(\"dad\")...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index test_id  sample_id sample_minor_id   \n",
       "0      0       0          0             NaN  \\\n",
       "1      1     1_a          1               a   \n",
       "2      2     1_b          1               b   \n",
       "3      3       2          2             NaN   \n",
       "4      4     3_a          3               a   \n",
       "\n",
       "                                                text   \n",
       "0  Check the availability of Pepsi at Walmart and...  \\\n",
       "1  If it's raining tomorrow morning, set my alarm...   \n",
       "2  If it's raining tomorrow morning, set my alarm...   \n",
       "3  Play the new Taylor Swift album and pull up my...   \n",
       "4        Send a message to dad if it rains tomorrow.   \n",
       "\n",
       "                                                code   \n",
       "0  product_name = ProductName.resolve_from_text(\"...  \\\n",
       "1  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "3  album = Album.resolve_from_text(\"the new Taylo...   \n",
       "4  date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "\n",
       "                                                test   \n",
       "0  # test data\\ndata_model = DataModel(reset=True...  \\\n",
       "1  # test data\\ndata_model = DataModel(reset=True...   \n",
       "2  # test data\\ndata_model = DataModel(reset=True...   \n",
       "3  # test data\\ndata_model = DataModel(reset=True...   \n",
       "4  # test data\\ndata_model = DataModel(reset=True...   \n",
       "\n",
       "                                             imports   \n",
       "0  from entities.generic import *\\nfrom entities....  \\\n",
       "1  from entities.generic import *\\nfrom entities....   \n",
       "2  from entities.generic import *\\nfrom entities....   \n",
       "3  from entities.generic import *\\nfrom entities....   \n",
       "4  from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                                            lang_rep   \n",
       "0  [ root [ S [ Command [ Action [ hd [ Check ] ]...  \\\n",
       "1  [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "2  [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "3  [ root [ S [ Command [ Action [ hd [ Play ] ] ...   \n",
       "4  [ root [ S [ Command [ Condition [ If [ Body [...   \n",
       "\n",
       "                                            code_rep   \n",
       "0  [ Module [ product_name = ProductName.resolve_...  \\\n",
       "1  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "2  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "3  [ Module [ album = Album.resolve_from_text('th...   \n",
       "4  [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "\n",
       "                                              output  \n",
       "0  product_name = ProductName.resolve_from_text(\"...  \n",
       "1  date_time = DateTime.resolve_from_text(\"tomorr...  \n",
       "2  date_time = DateTime.resolve_from_text(\"tomorr...  \n",
       "3  album_name = AlbumName.resolve_from_text(\"the ...  \n",
       "4  destination = Contact.resolve_from_text(\"dad\")...  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_oai_df = pd.read_csv(responses_file_path)\n",
    "eval_oai_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'humaneval': {'score': 0.08333333333333333,\n",
       "  'results':                            code_failure  correct  incorrect  accuracy\n",
       "  sample_id sample_minor_id                                            \n",
       "  0         NaN                         0        4          0       1.0\n",
       "  1         a                           1        0          0       0.0\n",
       "            b                           1        0          0       0.0\n",
       "  2         NaN                         1        0          0       0.0\n",
       "  3         a                           1        0          0       0.0\n",
       "  ...                                 ...      ...        ...       ...\n",
       "  104       b                           0        1          0       1.0\n",
       "  105       NaN                         1        0          0       0.0\n",
       "  108       NaN                         1        0          0       0.0\n",
       "  109       a                           1        0          0       0.0\n",
       "            b                           1        0          0       0.0\n",
       "  \n",
       "  [92 rows x 4 columns]},\n",
       " 'bleu': {'score': 0.4386397592741266,\n",
       "  'results':                            bleu_score\n",
       "  sample_id sample_minor_id            \n",
       "  0         NaN                0.641628\n",
       "  1         a                  0.535784\n",
       "            b                  0.521357\n",
       "  2         NaN                0.437082\n",
       "  3         a                  0.413056\n",
       "  ...                               ...\n",
       "  104       b                  0.679365\n",
       "  105       NaN                0.299206\n",
       "  108       NaN                0.365651\n",
       "  109       a                  0.310716\n",
       "            b                  0.291676\n",
       "  \n",
       "  [92 rows x 1 columns]}}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval(\n",
    "    responses_file_path,\n",
    "    compute_humanval=True, \n",
    "    compute_bleu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = responses_file_path\n",
    "parse_to_code = False\n",
    "output_column = 'output'\n",
    "\n",
    "results_df = pd.read_csv(results_file_path, compression='gzip')\n",
    "\n",
    "results_df['sample_id'] = results_df['sample_id'].astype(int)\n",
    "results_df.set_index(['sample_id', 'sample_minor_id'], inplace=True)\n",
    "results_df.sort_index(inplace=True)\n",
    "\n",
    "code_column = 'generated_code'\n",
    "results_df[code_column] = results_df[output_column]\n",
    "    \n",
    "results_df['test'] = results_df['test'].str.replace(\"= next(iterator)\", \"= next(iterator, None)\")\n",
    "results_df[code_column] = results_df[code_column].str.replace(\" = ContentType.\", \" = MessageContentType.\")\n",
    "results_df[code_column] = results_df[code_column].str.replace(\"Message.\", \"Messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4386397592741266,\n",
       " 'results':                            bleu_score\n",
       " sample_id sample_minor_id            \n",
       " 0         NaN                0.641628\n",
       " 1         a                  0.535784\n",
       "           b                  0.521357\n",
       " 2         NaN                0.437082\n",
       " 3         a                  0.413056\n",
       " ...                               ...\n",
       " 104       b                  0.679365\n",
       " 105       NaN                0.299206\n",
       " 108       NaN                0.365651\n",
       " 109       a                  0.310716\n",
       "           b                  0.291676\n",
       " \n",
       " [92 rows x 1 columns]}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = results_df\n",
    "gold_column = 'code'\n",
    "generated_column = 'generated_code'\n",
    "score_id_labels = 'sample_id'\n",
    "score_column_name: str = 'bleu_score'\n",
    "\n",
    "eval_results = data.apply(lambda x: eval_bleu(x[gold_column], x[generated_column]), axis=1)\n",
    "eval_results_df = eval_results.to_frame('bleu_score')\n",
    "score = eval_results_df.reset_index(drop=False).groupby(score_id_labels)[score_column_name].mean().mean()\n",
    "dict(score=score, results=eval_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_oai_df.set_index(['sample_id', 'sample_minor_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time = DateTime.resolve_from_text(\"tomorrow morning\")\n",
      "weather_forecasts = Weather.find_weather_forecasts(date_time=date_time)\n",
      "test_rain = any((weather_forecast.weather_type for weather_forecast in weather_forecasts) == Rain)\n",
      "te_est_weather_forecasts = bool(weather_forecasts)\n",
      "Responder.respond(response=test_weather_forecasts)\n",
      "if test_weather_forecasts and test_rain:\n",
      "  date_time = DateTime.resolve_from_text(\"7:30\")\n",
      "  Alarm.create_alarm(date_time=date_time)\n",
      "else:\n",
      "  date_time = DateTime.resolve_from_text(\"8\")\n",
      "  Alarm.create_alarm(date_time=date_time)\n"
     ]
    }
   ],
   "source": [
    "print(eval_oai_df['output'].loc[(1, 'a')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = DateTime.resolve_from_text(\"tomorrow morning\")\n",
    "weather_forecasts = Weather.find_weather_forecasts(date_time=date_time)\n",
    "test_rain = any((weather_forecast.weather_type for weather_forecast in weather_forecasts) == Rain)\n",
    "te_est_weather_forecasts = bool(weather_forecasts)\n",
    "Responder.respond(response=test_weather_forecasts)\n",
    "if test_weather_forecasts and test_rain:\n",
    "  date_time = DateTime.resolve_from_text(\"7:30\")\n",
    "  Alarm.create_alarm(date_time=date_time)\n",
    "else:\n",
    "  date_time = DateTime.resolve_from_text(\"8\")\n",
    "  Alarm.create_alarm(date_time=date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_name = EventName.resolve_from_text(\"the art festival\")\n",
      "date_time = DateTime.resolve_from_text(\"this weekend\")\n",
      "events = Calendar.find_events(event_name=event_name, date_time=date_time)\n",
      "Tickets.purchase_tickets(events=events)\n",
      "\n",
      "address = Address.resolve_from_text(\"the address\")\n",
      "Navigation.add_address_to_navigation(address=address)\n"
     ]
    }
   ],
   "source": [
    "print(eval_oai_df['output'].loc[(105, None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name = EventName.resolve_from_text(\"the art festival\")\n",
    "date_time = DateTime.resolve_from_text(\"this weekend\")\n",
    "events = Calendar.find_events(event_name=event_name, date_time=date_time)\n",
    "Tickets.purchase_tickets(events=events)\n",
    "\n",
    "address = Address.resolve_from_text(\"the address\")\n",
    "Navigation.add_address_to_navigation(address=address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_reminded = Contact.resolve_from_text(\"me\")\n",
      "date_time = DateTime.resolve_from_text(\"tomorrow\")\n",
      "contacts = Contact.resolve_many_from_text(\"Mom and Dad\")\n",
      "content = Content.resolve_from_text(\"send an email to contacts\")\n",
      "Reminders.create_reminder(person_reminded=person_reminded, date_time=date_\n"
     ]
    }
   ],
   "source": [
    "print(eval_oai_df['output'].loc[(55, None)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep', 'generated_code'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0717aaf589904d5cb3083c0bca656866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = build_examples_prompt(examples_prompt, examples_df, limit=13)\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm_notebook(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.to_csv(f'../build/openai-{MODEL_NAME}-{file_name}', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     walmart_availability = Store.check_availabilit...\n",
       "1     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "2     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "3     music_source = MusicSource.resolve_from_text(\"...\n",
       "4     recipient = Recipient.resolve_from_text(\"Dad\")...\n",
       "                            ...                        \n",
       "77    spotify_playlist_name = \"lofi\"\\nMediaPlayer.pl...\n",
       "78    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "79    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "80    date_time_start = DateTime.resolve_from_text(\"...\n",
       "81    date_time_tomorrow = DateTime.resolve_from_tex...\n",
       "Name: choices, Length: 82, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep', 'generated_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL_NAME = 'text-gpt4'\n",
    "model = openai.Model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_legs</th>\n",
       "      <th>num_wings</th>\n",
       "      <th>num_specimen_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>falcon</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spider</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_legs  num_wings  num_specimen_seen\n",
       "falcon         2          2                 10\n",
       "dog            4          0                  2\n",
       "spider         8          0                  1\n",
       "fish           0          0                  8"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
    "                   'num_wings': [2, 0, 0, 0],\n",
    "                   'num_specimen_seen': [10, 2, 1, 8]},\n",
    "                  index=['falcon', 'dog', 'spider', 'fish'])\n",
    "df2 = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
    "                   'num_wings': [2, 0, 0, 0],\n",
    "                   'num_specimen_seen': [10, 2, 1, 8]},)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_legs</th>\n",
       "      <th>num_wings</th>\n",
       "      <th>num_specimen_seen</th>\n",
       "      <th>num_legs</th>\n",
       "      <th>num_wings</th>\n",
       "      <th>num_specimen_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>falcon</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spider</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_legs  num_wings  num_specimen_seen  num_legs  num_wings  \\\n",
       "falcon       2.0        2.0               10.0       NaN        NaN   \n",
       "dog          4.0        0.0                2.0       NaN        NaN   \n",
       "spider       8.0        0.0                1.0       NaN        NaN   \n",
       "fish         0.0        0.0                8.0       NaN        NaN   \n",
       "0            NaN        NaN                NaN       2.0        2.0   \n",
       "1            NaN        NaN                NaN       4.0        0.0   \n",
       "2            NaN        NaN                NaN       8.0        0.0   \n",
       "3            NaN        NaN                NaN       0.0        0.0   \n",
       "\n",
       "        num_specimen_seen  \n",
       "falcon                NaN  \n",
       "dog                   NaN  \n",
       "spider                NaN  \n",
       "fish                  NaN  \n",
       "0                    10.0  \n",
       "1                     2.0  \n",
       "2                     1.0  \n",
       "3                     8.0  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "module = evaluate.load(\"dvitel/codebleu\")\n",
    "src = 'class AcidicSwampOoze(MinionCard):    def __init__(self):        super().__init__(\"Acidic Swamp Ooze\", 2, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, battlecry=Battlecry(Destroy(), WeaponSelector(EnemyPlayer())))    def create_minion(self, player):        return Minion(3, 2)'\n",
    "tgt = 'class AcidSwampOoze(MinionCard):    def __init__(self):        super().__init__(\"Acidic Swamp Ooze\", 2, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, battlecry=Battlecry(Destroy(), WeaponSelector(EnemyPlayer())))    def create_minion(self, player):        return Minion(3, 2)'\n",
    "src = src.replace(\"\",\"\\n\")\n",
    "tgt = tgt.replace(\"\",\"\\n\")\n",
    "res = module.compute(predictions = [tgt], references = [[src]])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biu] *",
   "language": "python",
   "name": "conda-env-biu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
