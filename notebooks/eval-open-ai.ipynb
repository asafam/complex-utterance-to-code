{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asaf/Workspace/biu/complex-utterance-to-code/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "WORK_AREA = '/Users/asaf/Workspace/biu/complex-utterance-to-code'\n",
    "os.chdir(WORK_AREA)\n",
    "\n",
    "paths = ['./src/', './src/api/v6', './notebooks/src']\n",
    "for path in paths:\n",
    "    path = os.path.normcase(path)\n",
    "    if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import openai\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "import math\n",
    "import tokenize\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from llm.prompts import build_prompt\n",
    "from llm.open_ai import OpenAIAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai @ file:///home/conda/feedstock_root/build_artifacts/openai_1686159246812/work\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_API_ORG\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'eval_complex_utterance_to_code_with_intermediate_152_20231112.csv.gz'\n",
    "BASE_PATH = '/Users/asaf/Workspace/biu/complex-utterance-to-code/build'\n",
    "\n",
    "def load_eval_data(file_name: str = FILE_NAME, base_path: str = BASE_PATH, sample: int = 0, random_seed: int = 42) -> pd.DataFrame:\n",
    "    eval_df = pd.read_csv(os.path.join(base_path, file_name))\n",
    "    eval_df = eval_df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "    if sample > 0:\n",
    "        sample_ids = eval_df['sample_id'].unique().tolist()\n",
    "        random.seed(random_seed)\n",
    "        random_sample_ids = random.sample(sample_ids, sample)\n",
    "        eval_df = eval_df[eval_df['sample_id'].isin(random_sample_ids)]\n",
    "        \n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asaf/Workspace/biu/complex-utterance-to-code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>see if find my first reminders that I have a m...</td>\n",
       "      <td>person_reminded = Contact.resolve_from_text(\"m...</td>\n",
       "      <td>[ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Action\\n...</td>\n",
       "      <td>\\t[ Module\\n\\t\\t[ person_reminded = Contact.re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>create a reminder at mindnight to close the wi...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"mindni...</td>\n",
       "      <td>[ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Action\\n...</td>\n",
       "      <td>\\t[ Module\\n\\t\\t[ date_time = DateTime.resolve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>show route to my office from Northern Mariana ...</td>\n",
       "      <td>origin = Location.resolve_from_text(\"from Nort...</td>\n",
       "      <td>[ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Conditio...</td>\n",
       "      <td>\\t[ Module\\n\\t\\t[ origin = Location.resolve_fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text   \n",
       "0      0  see if find my first reminders that I have a m...  \\\n",
       "1      1  create a reminder at mindnight to close the wi...   \n",
       "2      2  show route to my office from Northern Mariana ...   \n",
       "\n",
       "                                                code   \n",
       "0  person_reminded = Contact.resolve_from_text(\"m...  \\\n",
       "1  date_time = DateTime.resolve_from_text(\"mindni...   \n",
       "2  origin = Location.resolve_from_text(\"from Nort...   \n",
       "\n",
       "                                            lang_rep   \n",
       "0  [ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Action\\n...  \\\n",
       "1  [ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Action\\n...   \n",
       "2  [ root\\n\\t[ S\\n\\t\\t[ Command\\n\\t\\t\\t[ Conditio...   \n",
       "\n",
       "                                            code_rep  \n",
       "0  \\t[ Module\\n\\t\\t[ person_reminded = Contact.re...  \n",
       "1  \\t[ Module\\n\\t\\t[ date_time = DateTime.resolve...  \n",
       "2  \\t[ Module\\n\\t\\t[ origin = Location.resolve_fr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'build/train_complex_utterance_to_code_with_intermediate_40k.csv.gz'\n",
    "examples_df = pd.read_csv(file_path)\n",
    "examples_df = examples_df.reset_index()  # make sure indexes pair with number of rows\n",
    "examples_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(\n",
    "    test_df: pd.DataFrame,\n",
    "    test_results_df: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    platform: str,\n",
    "    prompt_args: dict,\n",
    "    n: int,\n",
    "    results_file_path: str,\n",
    "    max_tokens: int = 512,\n",
    "    wait_time_in_seconds: int = 5,\n",
    "    step_size: int = 0,\n",
    "    platform_disabled: bool = False,\n",
    "    serialize_response: bool = True,\n",
    "    random_seed: int = 42\n",
    "):\n",
    "    open_api = OpenAIAPI(\n",
    "        organization=os.getenv(\"OPENAI_API_ORG\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    total_records = test_df.shape[0]\n",
    "    step_size = step_size if step_size > 0 else n\n",
    "    id_labels = test_df.index.names\n",
    "\n",
    "    # generate predictions\n",
    "    responses = []\n",
    "    print(f\"Generating predictions for {total_records} records\")\n",
    "    for i, (index, row)  in tqdm.notebook.tqdm(enumerate(test_df.iterrows()), total=total_records, desc=\"Processing records\"):   \n",
    "        if not test_results_df.empty and index in test_results_df.index and len(test_results_df.loc[index]) == n:\n",
    "            # if we have a result for this record, skip\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "        \n",
    "        # iterate with step size until we reach a total of n\n",
    "        batch_steps = list(range(0, n, step_size))\n",
    "        for j, k in tqdm.notebook.tqdm(enumerate(batch_steps), total=len(batch_steps), leave=False, desc=f\"Processing record {i}\"):\n",
    "            ns = list(np.arange(k, min(k + step_size, n)))\n",
    "            records_to_duplicate = test_df.loc[[index]] # Fetch the records\n",
    "\n",
    "            if ((not test_results_df.empty) and (index in test_results_df.index) and test_results_df.loc[index, 'n'].isin(ns).sum() == len(ns) * len(records_to_duplicate)) or platform_disabled:\n",
    "                # if we have a result for this record, skip\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "\n",
    "            # remove the ns we already have\n",
    "            if index in test_results_df.index:\n",
    "                ns = [x for x in ns if (test_results_df.loc[index, 'n'] != x).all()]\n",
    "            \n",
    "            # run the model, if we don't have a result\n",
    "            seed = 42 + i*len(batch_steps) + j\n",
    "            args = {**prompt_args, **{\"input_data\": row, \"seed\": seed}}\n",
    "            messages = build_prompt(**args)\n",
    "\n",
    "            serialize_id = '_'.join([f'{i}{str(j)}' for i, j in list(zip(id_labels, [index] if len(id_labels) == 1 else list(index)))])\n",
    "            strategy = args['strategy']\n",
    "            prompt_type = args['prompt_type']\n",
    "            model_id = model_name.lower().replace('-', '_').replace('/', '_')\n",
    "            response = open_api.chat_complete(\n",
    "                model_name=model_name, \n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                n=len(ns),\n",
    "                serialize_id=serialize_id,\n",
    "                serialize_path=f'build/results/{platform}/responses/{model_id}/{strategy}/{prompt_type}/'\n",
    "            )\n",
    "\n",
    "            outputs = [x['message']['content'] for x in response['choices']]\n",
    "\n",
    "            # duplicate the records\n",
    "            copies = len(ns)\n",
    "            duplicated_records = pd.concat([records_to_duplicate] * copies, ignore_index=False) # Duplicate the records \n",
    "            duplicated_records['output'] = outputs * len(records_to_duplicate)\n",
    "            duplicated_records['n'] = ns * len(records_to_duplicate)\n",
    "            duplicated_records['input_seed'] = [seed] * copies * len(records_to_duplicate)\n",
    "            test_results_df = pd.concat([test_results_df, duplicated_records], ignore_index=False) # Append the duplicated records back to the original DataFrame (optional)\n",
    "            \n",
    "            test_results_df.to_csv(results_file_path, index=True, compression='gzip')\n",
    "\n",
    "            time.sleep(wait_time_in_seconds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-3.5-turbo-16k', 'gpt-3.5-turbo-1106', 'dall-e-3', 'gpt-3.5-turbo-16k-0613', 'dall-e-2', 'text-embedding-3-large', 'whisper-1', 'tts-1-hd-1106', 'tts-1-hd', 'gpt-3.5-turbo', 'gpt-3.5-turbo-0125', 'gpt-4-0613', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-instruct-0914', 'gpt-4', 'tts-1', 'davinci-002', 'gpt-3.5-turbo-instruct', 'babbage-002', 'gpt-4-1106-preview', 'gpt-4-vision-preview', 'tts-1-1106', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'text-embedding-ada-002', 'text-embedding-3-small']\n"
     ]
    }
   ],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_API_ORG\")\n",
    "openai.api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    "oai_models = openai.Model.list()\n",
    "print([model_data['id'] for model_data in oai_models['data']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'gpt-3.5-turbo-1106'\n",
    "model_name = 'gpt-4-0125-preview'\n",
    "platform = 'open_ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(\n",
    "    model_name: str, \n",
    "    platform: str, \n",
    "    strategy: str, # 'text2rep', 'text2code'\n",
    "    prompt_type: str = 'apispec', # 'examples' or 'apispec'\n",
    "    examples_limit: int = 11,\n",
    "    step_size: int = 0,\n",
    "    n: int = 200,\n",
    "    sample: int = 0,\n",
    "    random_seed: int = 42,\n",
    "):\n",
    "    print(f\"Running evaluation for {model_name} on {platform}\")\n",
    "    id_labels = ['sample_id'] #['test_id', 'sample_id', 'sample_minor_id']\n",
    "    model_id = model_name.lower().replace('-', '_').replace('/', '_')\n",
    "\n",
    "    test_df = load_eval_data(sample=sample, random_seed=random_seed)\n",
    "    test_df.set_index(id_labels, inplace=True)\n",
    "    test_df.sort_index(inplace=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    results_file_path = f\"./build/results/{platform}/test-{str(test_df.shape[0])}-{platform}-{model_id}-{strategy}-n{n}-sample{sample}-{prompt_type}-ex{examples_limit}-seed{random_seed}-{timestamp}.csv.gz\"\n",
    "    if os.path.exists(results_file_path):\n",
    "        print(f\"Results file already exists: {results_file_path}\")\n",
    "        return\n",
    "    \n",
    "    test_results_df = pd.read_csv(results_file_path, compression='gzip') if os.path.exists(results_file_path) else pd.DataFrame()\n",
    "    if not os.path.exists(results_file_path):\n",
    "        print(f\"Results file does not exist: {results_file_path}\")\n",
    "        # raise ValueError(f\"Results file already exists: {results_file_path}\")\n",
    "    else:\n",
    "        test_results_df.set_index(id_labels, inplace=True)\n",
    "        test_results_df.sort_index(inplace=True)\n",
    "\n",
    "    print(f\"Results will be saved to {results_file_path}\")\n",
    "\n",
    "    generate_predictions(\n",
    "        test_df=test_df,\n",
    "        test_results_df=test_results_df,\n",
    "        model_name=model_name,\n",
    "        platform=platform,\n",
    "        prompt_args={\n",
    "            \"examples_df\": examples_df,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            \"strategy\": strategy,\n",
    "            \"examples_limit\": examples_limit,\n",
    "            \"flattened_prompt\": False,\n",
    "        },\n",
    "        n=n,\n",
    "        results_file_path=results_file_path,\n",
    "        max_tokens=512,\n",
    "        wait_time_in_seconds=20,\n",
    "        step_size=step_size,\n",
    "        platform_disabled=False,\n",
    "        serialize_response=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-0125-preview on open_ai\n",
      "Results file does not exist: ./build/results/open_ai/test-19-open_ai-gpt_4_0125_preview-text2rep-n200-sample15-apispec-ex11-seed50110-202403181720.csv.gz\n",
      "Results will be saved to ./build/results/open_ai/test-19-open_ai-gpt_4_0125_preview-text2rep-n200-sample15-apispec-ex11-seed50110-202403181720.csv.gz\n",
      "Generating predictions for 19 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c580c860775743eda810c63b3f3b5c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484b6aa9e6d4ffda94266795f2aabd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 0:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50589a3a73bc48feb1b49750bf5c5995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 1:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0c0a4a46bd4567b83cf352fff10e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 2:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326df1750a39428c90ebd63429f96e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 3:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc5c52230844b7fbdb6bf7f066c72eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 4:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ebe998a194394a12fa3465969f2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing record 5:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retries = 0      # Current retry count\n",
    "max_retries = 20\n",
    "\n",
    "while retries < max_retries:\n",
    "    try:\n",
    "        run_eval(\n",
    "            model_name=model_name, \n",
    "            platform=platform, \n",
    "            strategy='text2rep', \n",
    "            prompt_type='apispec', \n",
    "            examples_limit=11, \n",
    "            step_size=100,\n",
    "            n=200,\n",
    "            sample=15,\n",
    "            random_seed=random.randint(0, 100000)\n",
    "        )\n",
    "        print(\"run_eval succeeded\")\n",
    "        break  # Exit the loop if the method succeeds\n",
    "    except Exception as e:\n",
    "        print(f\"run_eval method failed: {e}\")\n",
    "        retries += 1\n",
    "        sleep_time = 60\n",
    "        print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "        else:\n",
    "            print(\"Maximum retries reached, giving up.\")\n",
    "\n",
    "if retries == max_retries:\n",
    "    print(\"Failed after maximum retries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_file_path = f\"./build/results/open_ai/test-21-open_ai-gpt_3.5_turbo_1106-text2code-n200-sample15-apispec-ex11.csv.gz\"\n",
    "results_file_path = f\"./build/results/open_ai/test-21-open_ai-gpt_4_0125_preview-text2code-n200-sample15-apispec-ex11-202403181619.csv.gz\"\n",
    "results_file_path = f\"./build/results/open_ai/test-18-open_ai-gpt_4_0125_preview-text2code-n200-sample15-apispec-ex11-seed81537-202403181659.csv.gz\"\n",
    "results_file_path = f\"./build/results/open_ai/test-19-open_ai-gpt_4_0125_preview-text2rep-n200-sample15-apispec-ex11-seed50110-202403181720.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./build/results/open_ai/test-18-open_ai-gpt_4_0125_preview-text2code-n200-sample15-apispec-ex11-seed81537-202403181659.csv.gz\n",
      "In humaneval_accuracy_score...\n",
      "Evaluating test codes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1cfb75909b4147b9559e7526523cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass@1.mean = 0.36199999999999993\n",
      "pass@1.std = 0.4360406272028199\n",
      "\n",
      "pass@1.failed_results_pct = 0.5333333333333333\n",
      "pass@1.success_results_pct = 0.2\n",
      "\n",
      "pass@10.mean = 0.4661822332503201\n",
      "pass@10.std = 0.5158633711750059\n",
      "\n",
      "pass@10.failed_results_pct = 0.5333333333333333\n",
      "pass@10.success_results_pct = 0.2\n",
      "\n",
      "failed_results_pct = 0.6927777777777778\n",
      "success_results_pct = 0.30722222222222223\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_utils import model_eval\n",
    "\n",
    "\n",
    "print(f'Processing {results_file_path}')\n",
    "\n",
    "parse_to_code = results_file_path.find('2rep') > 0\n",
    "result, results_df = model_eval(\n",
    "    results_file_path=results_file_path, \n",
    "    parse_to_code=parse_to_code,\n",
    "    parse_rules_enabled=True,\n",
    ")\n",
    "\n",
    "for pass_k in result['humaneval']:\n",
    "    print(f\"{pass_k}.mean = {result['humaneval'][pass_k].mean()}\")\n",
    "    print(f\"{pass_k}.std = {result['humaneval'][pass_k].std()}\")\n",
    "    print()\n",
    "    failed_results_pct = (result['humaneval'][pass_k] == 0).sum()/len(result['humaneval'][pass_k])\n",
    "    print(f\"{pass_k}.failed_results_pct = {failed_results_pct}\")\n",
    "    success_results_pct = (result['humaneval'][pass_k] == 1).sum()/len(result['humaneval'][pass_k])\n",
    "    print(f\"{pass_k}.success_results_pct = {success_results_pct}\")\n",
    "    print()\n",
    "\n",
    "total_results = len(results_df)\n",
    "total_failed_results = (results_df['accuracy'] == 0).sum()\n",
    "total_success_results = (results_df['accuracy'] == 1).sum()\n",
    "print(f\"failed_results_pct = {total_failed_results / total_results}\")\n",
    "print(f\"success_results_pct = {total_success_results / total_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "./build/results/open_ai/test-21-open_ai-gpt_4_0125_preview-text2code-n200-sample15-apispec-ex11-202403181619.csv.gz\n",
    "\n",
    "pass@1.mean = 0.38733333333333325\n",
    "pass@1.std = 0.4200249425700245\n",
    "\n",
    "pass@1.failed_results_pct = 0.4\n",
    "pass@1.success_results_pct = 0.13333333333333333\n",
    "\n",
    "pass@10.mean = 0.5366838121575748\n",
    "pass@10.std = 0.49673532832914213\n",
    "\n",
    "pass@10.failed_results_pct = 0.4\n",
    "pass@10.success_results_pct = 0.13333333333333333\n",
    "\n",
    "failed_results_pct = 0.714047619047619\n",
    "success_results_pct = 0.28595238095238096\n",
    "\n",
    "\n",
    "./build/results/open_ai/test-18-open_ai-gpt_4_0125_preview-text2code-n200-sample15-apispec-ex11-seed81537-202403181659.csv.gz\n",
    "\n",
    "pass@1.mean = 0.36199999999999993\n",
    "pass@1.std = 0.4360406272028199\n",
    "\n",
    "pass@1.failed_results_pct = 0.5333333333333333\n",
    "pass@1.success_results_pct = 0.2\n",
    "\n",
    "pass@10.mean = 0.4661822332503201\n",
    "pass@10.std = 0.5158633711750059\n",
    "\n",
    "pass@10.failed_results_pct = 0.5333333333333333\n",
    "pass@10.success_results_pct = 0.2\n",
    "\n",
    "failed_results_pct = 0.6927777777777778\n",
    "success_results_pct = 0.30722222222222223\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
