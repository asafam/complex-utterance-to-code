{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "WORK_AREA = '..'\n",
    "os.chdir(WORK_AREA)\n",
    "\n",
    "paths = ['./src/', './src/api/v6', './notebooks/src']\n",
    "for path in paths:\n",
    "    path = os.path.normcase(path)\n",
    "    if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datetime import datetime\n",
    "from eval.utils import model_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>imports</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sample_minor_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>NaN</th>\n",
       "      <td>0</td>\n",
       "      <td>Check the availability of Pepsi at Walmart and...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Check ] ]...</td>\n",
       "      <td>[ Module [ product_name = ProductName.resolve_...</td>\n",
       "      <td>code:\\nproduct_name = ProductName.resolve_from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>10</th>\n",
       "      <th>NaN</th>\n",
       "      <td>12</td>\n",
       "      <td>Set a timer for one hour and text Stacy that d...</td>\n",
       "      <td>duration = DateTime.resolve_from_text(\"one hou...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Set ] ] [...</td>\n",
       "      <td>[ Module [ duration = DateTime.resolve_from_te...</td>\n",
       "      <td>code:\\nduration = Duration.resolve_from_text(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <th>102</th>\n",
       "      <th>NaN</th>\n",
       "      <td>85</td>\n",
       "      <td>Set an alarm for 7:30am and notify me with a r...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"7:30am...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Set ] ] [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>code:\\ndate_time = DateTime.resolve_from_text(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104_a</th>\n",
       "      <th>104</th>\n",
       "      <th>a</th>\n",
       "      <td>86</td>\n",
       "      <td>In the event that Jessica messages with road c...</td>\n",
       "      <td>sender = Contact.resolve_from_text(\"Jessica\")\\...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ Arg [ obl [ ca...</td>\n",
       "      <td>[ Module [ sender = Contact.resolve_from_text(...</td>\n",
       "      <td>code:\\nmessage_content_type = MessageMessageCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104_b</th>\n",
       "      <th>104</th>\n",
       "      <th>b</th>\n",
       "      <td>87</td>\n",
       "      <td>In the event that Jessica messages with road c...</td>\n",
       "      <td>sender = Contact.resolve_from_text(\"Jessica\")\\...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ Arg [ obl [ ca...</td>\n",
       "      <td>[ Module [ sender = Contact.resolve_from_text(...</td>\n",
       "      <td>code:\\nsender = Contact.resolve_from_text(\"Jes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   index   \n",
       "test_id sample_id sample_minor_id          \n",
       "0       0         NaN                  0  \\\n",
       "10      10        NaN                 12   \n",
       "102     102       NaN                 85   \n",
       "104_a   104       a                   86   \n",
       "104_b   104       b                   87   \n",
       "\n",
       "                                                                                text   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              Check the availability of Pepsi at Walmart and...  \\\n",
       "10      10        NaN              Set a timer for one hour and text Stacy that d...   \n",
       "102     102       NaN              Set an alarm for 7:30am and notify me with a r...   \n",
       "104_a   104       a                In the event that Jessica messages with road c...   \n",
       "104_b   104       b                In the event that Jessica messages with road c...   \n",
       "\n",
       "                                                                                code   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              product_name = ProductName.resolve_from_text(\"...  \\\n",
       "10      10        NaN              duration = DateTime.resolve_from_text(\"one hou...   \n",
       "102     102       NaN              date_time = DateTime.resolve_from_text(\"7:30am...   \n",
       "104_a   104       a                sender = Contact.resolve_from_text(\"Jessica\")\\...   \n",
       "104_b   104       b                sender = Contact.resolve_from_text(\"Jessica\")\\...   \n",
       "\n",
       "                                                                                test   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              # test data\\ndata_model = DataModel(reset=True...  \\\n",
       "10      10        NaN              # test data\\ndata_model = DataModel(reset=True...   \n",
       "102     102       NaN              # test data\\ndata_model = DataModel(reset=True...   \n",
       "104_a   104       a                # test data\\ndata_model = DataModel(reset=True...   \n",
       "104_b   104       b                # test data\\ndata_model = DataModel(reset=True...   \n",
       "\n",
       "                                                                             imports   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              from entities.generic import *\\nfrom entities....  \\\n",
       "10      10        NaN              from entities.generic import *\\nfrom entities....   \n",
       "102     102       NaN              from entities.generic import *\\nfrom entities....   \n",
       "104_a   104       a                from entities.generic import *\\nfrom entities....   \n",
       "104_b   104       b                from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                                                                            lang_rep   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              [ root [ S [ Command [ Action [ hd [ Check ] ]...  \\\n",
       "10      10        NaN              [ root [ S [ Command [ Action [ hd [ Set ] ] [...   \n",
       "102     102       NaN              [ root [ S [ Command [ Action [ hd [ Set ] ] [...   \n",
       "104_a   104       a                [ root [ S [ Command [ Action [ Arg [ obl [ ca...   \n",
       "104_b   104       b                [ root [ S [ Command [ Action [ Arg [ obl [ ca...   \n",
       "\n",
       "                                                                            code_rep   \n",
       "test_id sample_id sample_minor_id                                                      \n",
       "0       0         NaN              [ Module [ product_name = ProductName.resolve_...  \\\n",
       "10      10        NaN              [ Module [ duration = DateTime.resolve_from_te...   \n",
       "102     102       NaN              [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "104_a   104       a                [ Module [ sender = Contact.resolve_from_text(...   \n",
       "104_b   104       b                [ Module [ sender = Contact.resolve_from_text(...   \n",
       "\n",
       "                                                                              output  \n",
       "test_id sample_id sample_minor_id                                                     \n",
       "0       0         NaN              code:\\nproduct_name = ProductName.resolve_from...  \n",
       "10      10        NaN              code:\\nduration = Duration.resolve_from_text(\"...  \n",
       "102     102       NaN              code:\\ndate_time = DateTime.resolve_from_text(...  \n",
       "104_a   104       a                code:\\nmessage_content_type = MessageMessageCo...  \n",
       "104_b   104       b                code:\\nsender = Contact.resolve_from_text(\"Jes...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_labels = ['test_id', 'sample_id', 'sample_minor_id']\n",
    "results_df = pd.read_csv('./build/openai-gpt-3.5-turbo-16k-prompt-examples-k1-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "results_df.set_index(id_labels, inplace=True)\n",
    "results_df.sort_index(inplace=True)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.05327868852459016\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv('./build/openai-gpt-3.5-turbo-16k-prompt-apispec-k1-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "\n",
    "results = model_eval(\n",
    "    results_df=results_df,\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=False,\n",
    "    output_column='output',\n",
    "    gold_column='code',\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "# print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.04918032786885246\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv('./build/openai-gpt-3.5-turbo-16k-prompt-examples-k1-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "\n",
    "results = model_eval(\n",
    "    results_df=results_df,\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=False,\n",
    "    output_column='output',\n",
    "    gold_column='code',\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "# print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.1762295081967213\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv('./build/openai-gpt-3.5-turbo-16k-prompt-apispec-k10-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "\n",
    "results = model_eval(\n",
    "    results_df=results_df,\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=False,\n",
    "    output_column='output',\n",
    "    gold_column='code',\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "# print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.16393442622950818\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv('./build/openai-gpt-3.5-turbo-16k-prompt-examples-k10-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "\n",
    "results = model_eval(\n",
    "    results_df=results_df,\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=False,\n",
    "    output_column='output',\n",
    "    gold_column='code',\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "# print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval = 0.05737704918032787\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv('./build/openai-gpt-4-prompt-examples-k1-eval_complex_utterance_to_code_with_intermediate_152_20230525.csv.gz')\n",
    "\n",
    "results = model_eval(\n",
    "    results_df=results_df,\n",
    "    parse_to_code=False,\n",
    "    compute_humanval=True,\n",
    "    compute_bleu=False,\n",
    "    output_column='output',\n",
    "    gold_column='code',\n",
    ")\n",
    "print(f\"humaneval = {results['humaneval']['score']}\")\n",
    "# print(f\"bleu = {results['bleu']['score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b5679f7814cd6a8d4e6f405c305d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 10:51:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-05-13 10:51:52 INFO: File exists: /Users/asaf/stanza_resources/en/default.zip\n",
      "2023-05-13 10:51:55 INFO: Finished downloading models and saved to /Users/asaf/stanza_resources.\n",
      "2023-05-13 10:51:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanza version: 1.4.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441eb7138b1441ffa447461189b81b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 10:51:55 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-05-13 10:51:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-05-13 10:51:56 INFO: Use device: cpu\n",
      "2023-05-13 10:51:56 INFO: Loading: tokenize\n",
      "2023-05-13 10:51:56 INFO: Loading: pos\n",
      "2023-05-13 10:51:56 INFO: Loading: lemma\n",
      "2023-05-13 10:51:56 INFO: Loading: depparse\n",
      "2023-05-13 10:51:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza parser created\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete (40000/40000)\n",
      "Succesfully saved samples to build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generator.main(\n",
    "    k=40000, \n",
    "    print_console=False, \n",
    "    lang_representations=True, \n",
    "    code_representations=True, \n",
    "    output_file='build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz',\n",
    "    grammar_dir='config/grammar_messages_reminders_weather',\n",
    "    seed=42, \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build prompt from the API docstrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_spec_prompt(prompt_files_regexp='./config/prompt/**/*.txt'):\n",
    "    prompt_dict = {}\n",
    "    prompt_files = glob.glob(prompt_files_regexp)\n",
    "    for prompt_file in prompt_files:\n",
    "        key = os.path.basename(prompt_file).split('.')[0].lower()\n",
    "        with open(prompt_file, \"r\") as f:\n",
    "            prompt_dict[key] = f.read()\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for key, value in prompt_dict.items():\n",
    "        prompt = prompt + f\"# {key.upper()}:\\n\\n{value}\\n\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_prompt = build_spec_prompt(prompt_files_regexp='./config/prompts_messages_reminders_weather/**/*.txt')\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(spec_prompt, max_length=51200, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples prompt**\n",
    "\n",
    "\n",
    "Building a prompt from the text and code examples generated by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz'\n",
    "examples_df = pd.read_csv(file_path)\n",
    "examples_df = examples_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example_prompt(text, code=None):\n",
    "    examples_prompt = f\"text: \\n{text}\\n\\n\"\n",
    "    examples_prompt += f\"code: \\n{code}\\n\\n\\n\" if code else f\"code: \\n\"\n",
    "    \n",
    "    return examples_prompt\n",
    "\n",
    "\n",
    "def build_examples_prompt(base_prompt, df, limit=10):\n",
    "    examples_prompt = base_prompt or \"\"\n",
    "    for index, row in df[:limit].iterrows():\n",
    "        examples_prompt += build_example_prompt(text=row['text'], code=row['code'])\n",
    "    \n",
    "    return examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2903"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "examples_prompt = build_examples_prompt(base_examples_prompt, examples_df, limit=10)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(examples_prompt, max_length=51200, truncation=True)[\"input_ids\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'eval_complex_utterance_to_code_with_intermediate_82_20230509.csv.gz'\n",
    "base_path = '/Users/asaf/Workspace/biu/complex-utterance-to-code/build'\n",
    "eval_df = pd.read_csv(os.path.join(base_path, file_name))\n",
    "eval_df = eval_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"code\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_code(code: str, imports: str, test: str, code_embed_str: str = '# end code block to test', fail_on_error: bool = False, verbose: str = 'Fatal'):\n",
    "  try:\n",
    "    code_insert_idx = test.find(code_embed_str)\n",
    "    program_code = imports\n",
    "    program_code += '\\n'\n",
    "    program_code += test[:code_insert_idx]\n",
    "    program_code += code\n",
    "    program_code += '\\n'\n",
    "    program_code += test[code_insert_idx:]\n",
    "  except Exception as e:\n",
    "    if verbose == 'Error':\n",
    "      print('[ERROR] Failed to unparse code rep to code\\n', e)\n",
    "    if fail_on_error:\n",
    "      raise e\n",
    "    program_code = ''\n",
    "  finally:\n",
    "    return program_code\n",
    "\n",
    "def eval_code(code: str):\n",
    "  test_results = {}\n",
    "  try:\n",
    "    context = {}\n",
    "    exec(code, context)\n",
    "    test_results = context.get('test_results', {})\n",
    "  except AssertionError as e:\n",
    "    test_results['test_failuers'] = test_results.get('test_failuers', 0) + 1\n",
    "  except Exception as e:\n",
    "    test_results['code_failure'] = test_results.get('code_failure', 0) + 1\n",
    "  \n",
    "  return test_results\n",
    "  \n",
    "\n",
    "def compute_scores(df, index):\n",
    "  # df = pd.DataFrame(data)\n",
    "  df = df.set_index(index).sort_index()\n",
    "  df['correct'] = df['results'].apply(lambda test_results: test_results.get('correct', 0))\n",
    "  df['incorrect'] = df['results'].apply(lambda test_results: test_results.get('incorrect', 0))\n",
    "  df['total'] = df['correct'] + df['incorrect']\n",
    "  df['code_failure'] = df['results'].apply(lambda test_results: test_results.get('code_failure', 0))\n",
    "  df['score'] = ((1 - df['code_failure']) * (df['correct'] / df['total'])).replace({np.nan: 0})\n",
    "\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage', 'davinci', 'text-davinci-edit-001', 'whisper-1', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'babbage-code-search-text', 'babbage-similarity', 'code-search-babbage-text-001', 'text-curie-001', 'code-search-babbage-code-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'curie-instruct-beta', 'gpt-3.5-turbo', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'gpt-3.5-turbo-0301', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'gpt-4-0314', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'gpt-4', 'curie-search-document', 'text-davinci-003', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n"
     ]
    }
   ],
   "source": [
    "oai_models = openai.Model.list()\n",
    "print([model_data['id'] for model_data in oai_models['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage-code-search-code', 'code-davinci-edit-001', 'babbage-code-search-text', 'code-search-babbage-text-001', 'code-search-babbage-code-001', 'ada-code-search-code', 'code-search-ada-text-001', 'ada-code-search-text', 'code-search-ada-code-001']\n"
     ]
    }
   ],
   "source": [
    "print([model_data['id'] for model_data in oai_models['data'] if 'code' in model_data['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Model model id=babbage at 0x166040810> JSON: {\n",
       "  \"created\": 1649358449,\n",
       "  \"id\": \"babbage\",\n",
       "  \"object\": \"model\",\n",
       "  \"owned_by\": \"openai\",\n",
       "  \"parent\": null,\n",
       "  \"permission\": [\n",
       "    {\n",
       "      \"allow_create_engine\": false,\n",
       "      \"allow_fine_tuning\": false,\n",
       "      \"allow_logprobs\": true,\n",
       "      \"allow_sampling\": true,\n",
       "      \"allow_search_indices\": false,\n",
       "      \"allow_view\": true,\n",
       "      \"created\": 1669085501,\n",
       "      \"group\": null,\n",
       "      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n",
       "      \"is_blocking\": false,\n",
       "      \"object\": \"model_permission\",\n",
       "      \"organization\": \"*\"\n",
       "    }\n",
       "  ],\n",
       "  \"root\": \"babbage\"\n",
       "}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'\n",
    "timestamp_str = datetime.now().strftime('%Y-%m-%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_example_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = spec_prompt\n",
    "base_prompt += '\\n'\n",
    "base_prompt += build_examples_prompt(base_example_prompt, examples_df, limit=3)\n",
    "\n",
    "len(tokenizer(base_prompt, max_length=51200, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8548c764d4594e0ebcb141bb2e3d3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm.notebook.tqdm(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt + '\\n'\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.to_csv(f'build/openai-{MODEL_NAME}-{file_name}-{timestamp_str}', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     walmart_availability = Store.check_availabilit...\n",
       "1     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "2     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "3     music_source = MusicSource.resolve_from_text(\"...\n",
       "4     recipient = Recipient.resolve_from_text(\"Dad\")...\n",
       "                            ...                        \n",
       "77    spotify_playlist_name = \"lofi\"\\nMediaPlayer.pl...\n",
       "78    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "79    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "80    date_time_start = DateTime.resolve_from_text(\"...\n",
       "81    date_time_tomorrow = DateTime.resolve_from_tex...\n",
       "Name: choices, Length: 82, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0390625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_id</th>\n",
       "      <th>sample_minor_id</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>imports</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "      <th>generated_code</th>\n",
       "      <th>test_code</th>\n",
       "      <th>results</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>total</th>\n",
       "      <th>code_failure</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check the availability of Pepsi at Walmart and...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Check ] ]...</td>\n",
       "      <td>[ Module [ product_name = ProductName.resolve_...</td>\n",
       "      <td>content = Content.resolve_from_text(\"Pepsi\")\\n...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_a</td>\n",
       "      <td>a</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_b</td>\n",
       "      <td>b</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Play the new Taylor Swift album and pull up my...</td>\n",
       "      <td>album = Album.resolve_from_text(\"the new Taylo...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Play ] ] ...</td>\n",
       "      <td>[ Module [ album = Album.resolve_from_text('th...</td>\n",
       "      <td>album = Content.resolve_from_text('the new Tay...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3_a</td>\n",
       "      <td>a</td>\n",
       "      <td>Send a message to dad if it rains tomorrow.</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Body [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>recipient = Contact.resolve_from_text(\"dad\")\\n...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index test_id sample_minor_id  \\\n",
       "sample_id                                  \n",
       "0              0       0             NaN   \n",
       "1              1     1_a               a   \n",
       "1              2     1_b               b   \n",
       "2              3       2             NaN   \n",
       "3              4     3_a               a   \n",
       "\n",
       "                                                        text  \\\n",
       "sample_id                                                      \n",
       "0          Check the availability of Pepsi at Walmart and...   \n",
       "1          If it's raining tomorrow morning, set my alarm...   \n",
       "1          If it's raining tomorrow morning, set my alarm...   \n",
       "2          Play the new Taylor Swift album and pull up my...   \n",
       "3                Send a message to dad if it rains tomorrow.   \n",
       "\n",
       "                                                        code  \\\n",
       "sample_id                                                      \n",
       "0          product_name = ProductName.resolve_from_text(\"...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2          album = Album.resolve_from_text(\"the new Taylo...   \n",
       "3          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "\n",
       "                                                        test  \\\n",
       "sample_id                                                      \n",
       "0          # test data\\ndata_model = DataModel(reset=True...   \n",
       "1          # test data\\ndata_model = DataModel(reset=True...   \n",
       "1          # test data\\ndata_model = DataModel(reset=True...   \n",
       "2          # test data\\ndata_model = DataModel(reset=True...   \n",
       "3          # test data\\ndata_model = DataModel(reset=True...   \n",
       "\n",
       "                                                     imports  \\\n",
       "sample_id                                                      \n",
       "0          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "2          from entities.generic import *\\nfrom entities....   \n",
       "3          from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                                                    lang_rep  \\\n",
       "sample_id                                                      \n",
       "0          [ root [ S [ Command [ Action [ hd [ Check ] ]...   \n",
       "1          [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "1          [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "2          [ root [ S [ Command [ Action [ hd [ Play ] ] ...   \n",
       "3          [ root [ S [ Command [ Condition [ If [ Body [...   \n",
       "\n",
       "                                                    code_rep  \\\n",
       "sample_id                                                      \n",
       "0          [ Module [ product_name = ProductName.resolve_...   \n",
       "1          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "1          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "2          [ Module [ album = Album.resolve_from_text('th...   \n",
       "3          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "\n",
       "                                              generated_code  \\\n",
       "sample_id                                                      \n",
       "0          content = Content.resolve_from_text(\"Pepsi\")\\n...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2          album = Content.resolve_from_text('the new Tay...   \n",
       "3          recipient = Contact.resolve_from_text(\"dad\")\\n...   \n",
       "\n",
       "                                                   test_code  \\\n",
       "sample_id                                                      \n",
       "0          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "2          from entities.generic import *\\nfrom entities....   \n",
       "3          from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                       results  correct  incorrect  total  code_failure  score  \n",
       "sample_id                                                                       \n",
       "0          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "1          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "1          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "2          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "3          {'code_failure': 1}        0          0      0             1    0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77\n",
       "0     5\n",
       "Name: code_failure, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df['code_failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct  incorrect\n",
       "0        0            78\n",
       "1        0             2\n",
       "3        0             1\n",
       "6        0             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[['correct', 'incorrect']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0717aaf589904d5cb3083c0bca656866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = build_examples_prompt(examples_prompt, examples_df, limit=13)\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm_notebook(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.to_csv(f'../build/openai-{MODEL_NAME}-{file_name}', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     walmart_availability = Store.check_availabilit...\n",
       "1     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "2     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "3     music_source = MusicSource.resolve_from_text(\"...\n",
       "4     recipient = Recipient.resolve_from_text(\"Dad\")...\n",
       "                            ...                        \n",
       "77    spotify_playlist_name = \"lofi\"\\nMediaPlayer.pl...\n",
       "78    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "79    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "80    date_time_start = DateTime.resolve_from_text(\"...\n",
       "81    date_time_tomorrow = DateTime.resolve_from_tex...\n",
       "Name: choices, Length: 82, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep', 'generated_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL_NAME = 'text-gpt4'\n",
    "model = openai.Model(MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biu] *",
   "language": "python",
   "name": "conda-env-biu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
