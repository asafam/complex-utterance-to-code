{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "WORK_AREA = \"../\"\n",
    "os.chdir(WORK_AREA)\n",
    "\n",
    "paths = ['./src/', './src/api/v6']\n",
    "for path in paths:\n",
    "    path = os.path.normcase(path)\n",
    "    if not any(os.path.normcase(sp) == path for sp in sys.path):\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_API_ORG\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b5679f7814cd6a8d4e6f405c305d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 10:51:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-05-13 10:51:52 INFO: File exists: /Users/asaf/stanza_resources/en/default.zip\n",
      "2023-05-13 10:51:55 INFO: Finished downloading models and saved to /Users/asaf/stanza_resources.\n",
      "2023-05-13 10:51:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanza version: 1.4.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441eb7138b1441ffa447461189b81b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 10:51:55 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-05-13 10:51:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-05-13 10:51:56 INFO: Use device: cpu\n",
      "2023-05-13 10:51:56 INFO: Loading: tokenize\n",
      "2023-05-13 10:51:56 INFO: Loading: pos\n",
      "2023-05-13 10:51:56 INFO: Loading: lemma\n",
      "2023-05-13 10:51:56 INFO: Loading: depparse\n",
      "2023-05-13 10:51:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza parser created\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete (40000/40000)\n",
      "Succesfully saved samples to build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generator.main(\n",
    "    k=40000, \n",
    "    print_console=False, \n",
    "    lang_representations=True, \n",
    "    code_representations=True, \n",
    "    output_file='build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz',\n",
    "    grammar_dir='config/grammar_messages_reminders_weather',\n",
    "    seed=42, \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build prompt from the API docstrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_spec_prompt(prompt_files_regexp='./config/prompt/**/*.txt'):\n",
    "    prompt_dict = {}\n",
    "    prompt_files = glob.glob(prompt_files_regexp)\n",
    "    for prompt_file in prompt_files:\n",
    "        key = os.path.basename(prompt_file).split('.')[0].lower()\n",
    "        with open(prompt_file, \"r\") as f:\n",
    "            prompt_dict[key] = f.read()\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for key, value in prompt_dict.items():\n",
    "        prompt = prompt + f\"# {key.upper()}:\\n\\n{value}\\n\\n\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_prompt = build_spec_prompt(prompt_files_regexp='./config/prompts_messages_reminders_weather/**/*.txt')\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(spec_prompt, max_length=51200, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples prompt**\n",
    "\n",
    "\n",
    "Building a prompt from the text and code examples generated by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'build/train_3domains_complex_utterance_to_code_with_intermediate_40k.csv.gz'\n",
    "examples_df = pd.read_csv(file_path)\n",
    "examples_df = examples_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example_prompt(text, code=None):\n",
    "    examples_prompt = f\"text: \\n{text}\\n\\n\"\n",
    "    examples_prompt += f\"code: \\n{code}\\n\\n\\n\" if code else f\"code: \\n\"\n",
    "    \n",
    "    return examples_prompt\n",
    "\n",
    "\n",
    "def build_examples_prompt(base_prompt, df, limit=10):\n",
    "    examples_prompt = base_prompt or \"\"\n",
    "    for index, row in df[:limit].iterrows():\n",
    "        examples_prompt += build_example_prompt(text=row['text'], code=row['code'])\n",
    "    \n",
    "    return examples_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2903"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "examples_prompt = build_examples_prompt(base_examples_prompt, examples_df, limit=10)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "len(tokenizer(examples_prompt, max_length=51200, truncation=True)[\"input_ids\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'eval_complex_utterance_to_code_with_intermediate_82_20230509.csv.gz'\n",
    "base_path = '/Users/asaf/Workspace/biu/complex-utterance-to-code/build'\n",
    "eval_df = pd.read_csv(os.path.join(base_path, file_name))\n",
    "eval_df = eval_df.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"code\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_code(code: str, imports: str, test: str, code_embed_str: str = '# end code block to test', fail_on_error: bool = False, verbose: str = 'Fatal'):\n",
    "  try:\n",
    "    code_insert_idx = test.find(code_embed_str)\n",
    "    program_code = imports\n",
    "    program_code += '\\n'\n",
    "    program_code += test[:code_insert_idx]\n",
    "    program_code += code\n",
    "    program_code += '\\n'\n",
    "    program_code += test[code_insert_idx:]\n",
    "  except Exception as e:\n",
    "    if verbose == 'Error':\n",
    "      print('[ERROR] Failed to unparse code rep to code\\n', e)\n",
    "    if fail_on_error:\n",
    "      raise e\n",
    "    program_code = ''\n",
    "  finally:\n",
    "    return program_code\n",
    "\n",
    "def eval_code(code: str):\n",
    "  test_results = {}\n",
    "  try:\n",
    "    context = {}\n",
    "    exec(code, context)\n",
    "    test_results = context.get('test_results', {})\n",
    "  except AssertionError as e:\n",
    "    test_results['test_failuers'] = test_results.get('test_failuers', 0) + 1\n",
    "  except Exception as e:\n",
    "    test_results['code_failure'] = test_results.get('code_failure', 0) + 1\n",
    "  \n",
    "  return test_results\n",
    "  \n",
    "\n",
    "def compute_scores(df, index):\n",
    "  # df = pd.DataFrame(data)\n",
    "  df = df.set_index(index).sort_index()\n",
    "  df['correct'] = df['results'].apply(lambda test_results: test_results.get('correct', 0))\n",
    "  df['incorrect'] = df['results'].apply(lambda test_results: test_results.get('incorrect', 0))\n",
    "  df['total'] = df['correct'] + df['incorrect']\n",
    "  df['code_failure'] = df['results'].apply(lambda test_results: test_results.get('code_failure', 0))\n",
    "  df['score'] = ((1 - df['code_failure']) * (df['correct'] / df['total'])).replace({np.nan: 0})\n",
    "\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage', 'davinci', 'text-davinci-edit-001', 'whisper-1', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'babbage-code-search-text', 'babbage-similarity', 'code-search-babbage-text-001', 'text-curie-001', 'code-search-babbage-code-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'curie-instruct-beta', 'gpt-3.5-turbo', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'gpt-3.5-turbo-0301', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'gpt-4-0314', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'gpt-4', 'curie-search-document', 'text-davinci-003', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n"
     ]
    }
   ],
   "source": [
    "oai_models = openai.Model.list()\n",
    "print([model_data['id'] for model_data in oai_models['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage-code-search-code', 'code-davinci-edit-001', 'babbage-code-search-text', 'code-search-babbage-text-001', 'code-search-babbage-code-001', 'ada-code-search-code', 'code-search-ada-text-001', 'ada-code-search-text', 'code-search-ada-code-001']\n"
     ]
    }
   ],
   "source": [
    "print([model_data['id'] for model_data in oai_models['data'] if 'code' in model_data['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Model model id=babbage at 0x166040810> JSON: {\n",
       "  \"created\": 1649358449,\n",
       "  \"id\": \"babbage\",\n",
       "  \"object\": \"model\",\n",
       "  \"owned_by\": \"openai\",\n",
       "  \"parent\": null,\n",
       "  \"permission\": [\n",
       "    {\n",
       "      \"allow_create_engine\": false,\n",
       "      \"allow_fine_tuning\": false,\n",
       "      \"allow_logprobs\": true,\n",
       "      \"allow_sampling\": true,\n",
       "      \"allow_search_indices\": false,\n",
       "      \"allow_view\": true,\n",
       "      \"created\": 1669085501,\n",
       "      \"group\": null,\n",
       "      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n",
       "      \"is_blocking\": false,\n",
       "      \"object\": \"model_permission\",\n",
       "      \"organization\": \"*\"\n",
       "    }\n",
       "  ],\n",
       "  \"root\": \"babbage\"\n",
       "}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'\n",
    "timestamp_str = datetime.now().strftime('%Y-%m-%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_example_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = spec_prompt\n",
    "base_prompt += '\\n'\n",
    "base_prompt += build_examples_prompt(base_example_prompt, examples_df, limit=3)\n",
    "\n",
    "len(tokenizer(base_prompt, max_length=51200, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8548c764d4594e0ebcb141bb2e3d3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm.notebook.tqdm(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt + '\\n'\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.to_csv(f'build/openai-{MODEL_NAME}-{file_name}-{timestamp_str}', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     walmart_availability = Store.check_availabilit...\n",
       "1     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "2     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "3     music_source = MusicSource.resolve_from_text(\"...\n",
       "4     recipient = Recipient.resolve_from_text(\"Dad\")...\n",
       "                            ...                        \n",
       "77    spotify_playlist_name = \"lofi\"\\nMediaPlayer.pl...\n",
       "78    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "79    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "80    date_time_start = DateTime.resolve_from_text(\"...\n",
       "81    date_time_tomorrow = DateTime.resolve_from_tex...\n",
       "Name: choices, Length: 82, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0390625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_id</th>\n",
       "      <th>sample_minor_id</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test</th>\n",
       "      <th>imports</th>\n",
       "      <th>lang_rep</th>\n",
       "      <th>code_rep</th>\n",
       "      <th>generated_code</th>\n",
       "      <th>test_code</th>\n",
       "      <th>results</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>total</th>\n",
       "      <th>code_failure</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check the availability of Pepsi at Walmart and...</td>\n",
       "      <td>product_name = ProductName.resolve_from_text(\"...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Check ] ]...</td>\n",
       "      <td>[ Module [ product_name = ProductName.resolve_...</td>\n",
       "      <td>content = Content.resolve_from_text(\"Pepsi\")\\n...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_a</td>\n",
       "      <td>a</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1_b</td>\n",
       "      <td>b</td>\n",
       "      <td>If it's raining tomorrow morning, set my alarm...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Test [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Play the new Taylor Swift album and pull up my...</td>\n",
       "      <td>album = Album.resolve_from_text(\"the new Taylo...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Action [ hd [ Play ] ] ...</td>\n",
       "      <td>[ Module [ album = Album.resolve_from_text('th...</td>\n",
       "      <td>album = Content.resolve_from_text('the new Tay...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3_a</td>\n",
       "      <td>a</td>\n",
       "      <td>Send a message to dad if it rains tomorrow.</td>\n",
       "      <td>date_time = DateTime.resolve_from_text(\"tomorr...</td>\n",
       "      <td># test data\\ndata_model = DataModel(reset=True...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>[ root [ S [ Command [ Condition [ If [ Body [...</td>\n",
       "      <td>[ Module [ date_time = DateTime.resolve_from_t...</td>\n",
       "      <td>recipient = Contact.resolve_from_text(\"dad\")\\n...</td>\n",
       "      <td>from entities.generic import *\\nfrom entities....</td>\n",
       "      <td>{'code_failure': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index test_id sample_minor_id  \\\n",
       "sample_id                                  \n",
       "0              0       0             NaN   \n",
       "1              1     1_a               a   \n",
       "1              2     1_b               b   \n",
       "2              3       2             NaN   \n",
       "3              4     3_a               a   \n",
       "\n",
       "                                                        text  \\\n",
       "sample_id                                                      \n",
       "0          Check the availability of Pepsi at Walmart and...   \n",
       "1          If it's raining tomorrow morning, set my alarm...   \n",
       "1          If it's raining tomorrow morning, set my alarm...   \n",
       "2          Play the new Taylor Swift album and pull up my...   \n",
       "3                Send a message to dad if it rains tomorrow.   \n",
       "\n",
       "                                                        code  \\\n",
       "sample_id                                                      \n",
       "0          product_name = ProductName.resolve_from_text(\"...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2          album = Album.resolve_from_text(\"the new Taylo...   \n",
       "3          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "\n",
       "                                                        test  \\\n",
       "sample_id                                                      \n",
       "0          # test data\\ndata_model = DataModel(reset=True...   \n",
       "1          # test data\\ndata_model = DataModel(reset=True...   \n",
       "1          # test data\\ndata_model = DataModel(reset=True...   \n",
       "2          # test data\\ndata_model = DataModel(reset=True...   \n",
       "3          # test data\\ndata_model = DataModel(reset=True...   \n",
       "\n",
       "                                                     imports  \\\n",
       "sample_id                                                      \n",
       "0          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "2          from entities.generic import *\\nfrom entities....   \n",
       "3          from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                                                    lang_rep  \\\n",
       "sample_id                                                      \n",
       "0          [ root [ S [ Command [ Action [ hd [ Check ] ]...   \n",
       "1          [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "1          [ root [ S [ Command [ Condition [ If [ Test [...   \n",
       "2          [ root [ S [ Command [ Action [ hd [ Play ] ] ...   \n",
       "3          [ root [ S [ Command [ Condition [ If [ Body [...   \n",
       "\n",
       "                                                    code_rep  \\\n",
       "sample_id                                                      \n",
       "0          [ Module [ product_name = ProductName.resolve_...   \n",
       "1          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "1          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "2          [ Module [ album = Album.resolve_from_text('th...   \n",
       "3          [ Module [ date_time = DateTime.resolve_from_t...   \n",
       "\n",
       "                                              generated_code  \\\n",
       "sample_id                                                      \n",
       "0          content = Content.resolve_from_text(\"Pepsi\")\\n...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "1          date_time = DateTime.resolve_from_text(\"tomorr...   \n",
       "2          album = Content.resolve_from_text('the new Tay...   \n",
       "3          recipient = Contact.resolve_from_text(\"dad\")\\n...   \n",
       "\n",
       "                                                   test_code  \\\n",
       "sample_id                                                      \n",
       "0          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "1          from entities.generic import *\\nfrom entities....   \n",
       "2          from entities.generic import *\\nfrom entities....   \n",
       "3          from entities.generic import *\\nfrom entities....   \n",
       "\n",
       "                       results  correct  incorrect  total  code_failure  score  \n",
       "sample_id                                                                       \n",
       "0          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "1          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "1          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "2          {'code_failure': 1}        0          0      0             1    0.0  \n",
       "3          {'code_failure': 1}        0          0      0             1    0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77\n",
       "0     5\n",
       "Name: code_failure, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df['code_failure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct  incorrect\n",
       "0        0            78\n",
       "1        0             2\n",
       "3        0             1\n",
       "6        0             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[['correct', 'incorrect']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'text-davinci-003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0717aaf589904d5cb3083c0bca656866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_prompt = \"\"\"\n",
    "Transform text to code\n",
    "\n",
    "# EXAMPLES:\n",
    "\n",
    "\"\"\"\n",
    "base_prompt = build_examples_prompt(examples_prompt, examples_df, limit=13)\n",
    "\n",
    "responses = []\n",
    "for i, row  in tqdm_notebook(eval_df.iterrows(), total=eval_df.shape[0], desc=\"Processing records\"):\n",
    "    prompt = base_prompt\n",
    "    prompt += build_example_prompt(text=row['text'])\n",
    "    \n",
    "    response = openai.Completion.create(engine=MODEL_NAME, prompt=prompt, max_tokens=1000)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.to_csv(f'../build/openai-{MODEL_NAME}-{file_name}', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     walmart_availability = Store.check_availabilit...\n",
       "1     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "2     date_time = DateTime.resolve_from_text(\"tomorr...\n",
       "3     music_source = MusicSource.resolve_from_text(\"...\n",
       "4     recipient = Recipient.resolve_from_text(\"Dad\")...\n",
       "                            ...                        \n",
       "77    spotify_playlist_name = \"lofi\"\\nMediaPlayer.pl...\n",
       "78    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "79    date_time = DateTime.resolve_from_text(\"tonigh...\n",
       "80    date_time_start = DateTime.resolve_from_text(\"...\n",
       "81    date_time_tomorrow = DateTime.resolve_from_tex...\n",
       "Name: choices, Length: 82, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'test_id', 'sample_id', 'sample_minor_id', 'text', 'code',\n",
       "       'test', 'imports', 'lang_rep', 'code_rep', 'generated_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['generated_code'] = responses_df['choices'].apply(lambda choices: choices[0]['text'] if choices else None)\n",
    "eval_df['test_code'] = eval_df.apply(lambda row: build_test_code(code=row['generated_code'], imports=row['imports'], test=row['test']), axis=1)\n",
    "eval_df['results'] = eval_df['test_code'].apply(lambda code: eval_code(code))\n",
    "\n",
    "scores_df = compute_scores(eval_df, index='sample_id')\n",
    "scores_df.groupby('sample_id')['score'].mean().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL_NAME = 'text-gpt4'\n",
    "model = openai.Model(MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biu] *",
   "language": "python",
   "name": "conda-env-biu-py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
